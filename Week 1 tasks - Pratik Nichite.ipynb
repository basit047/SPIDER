{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "- Stemming is chopping of word endings to get its root form."
      ],
      "metadata": {
        "id": "ssnJW6Ut8ykv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer"
      ],
      "metadata": {
        "id": "8YvMfigg1KDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68618e83-c78e-49a2-f5eb-631aef69dcea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The runner was running quickly, and it made him realize that he could run even faster.\""
      ],
      "metadata": {
        "id": "k091J0GX7VTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PorterStemmer:\n",
        "  - It's very basic, old and fast stemmer.\n",
        "2. SnowballStemmer (english stemmer):\n",
        "  - It offers slight improvement on porter stemmers logic and speed.\n",
        "3. LancasterStemmer:\n",
        "  - Simple but produces results with over stemming. (meaningless roots)\n",
        "4. RegexpStemmer:\n",
        "  - Uses regular expressions for stemming."
      ],
      "metadata": {
        "id": "oaL6k0odoq_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "ls = LancasterStemmer()\n",
        "ss = SnowballStemmer(language=\"english\")\n",
        "res = RegexpStemmer('ing$|s$|e$|able$|er$|ly$')"
      ],
      "metadata": {
        "id": "vl93tr-N7pZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnSUb-jv7x_F",
        "outputId": "3e54335a-8b77-4c70-8728-c785551c8604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'runner',\n",
              " 'was',\n",
              " 'running',\n",
              " 'quickly',\n",
              " ',',\n",
              " 'and',\n",
              " 'it',\n",
              " 'made',\n",
              " 'him',\n",
              " 'realize',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'run',\n",
              " 'even',\n",
              " 'faster',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "  print(f'token: \"{token}\"')\n",
        "  print(\"porter-\", ps.stem(token))\n",
        "  print(\"lancaster-\", ls.stem(token))\n",
        "  print(\"snowball-\", ss.stem(token))\n",
        "  print(\"regexp-\", res.stem(token))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfbKbvYv7y7P",
        "outputId": "0df00b44-39eb-4d17-baf3-4b8c97fcf4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: \"The\"\n",
            "porter- the\n",
            "lancaster- the\n",
            "snowball- the\n",
            "regexp- Th\n",
            "\n",
            "token: \"runner\"\n",
            "porter- runner\n",
            "lancaster- run\n",
            "snowball- runner\n",
            "regexp- runn\n",
            "\n",
            "token: \"was\"\n",
            "porter- wa\n",
            "lancaster- was\n",
            "snowball- was\n",
            "regexp- wa\n",
            "\n",
            "token: \"running\"\n",
            "porter- run\n",
            "lancaster- run\n",
            "snowball- run\n",
            "regexp- runn\n",
            "\n",
            "token: \"quickly\"\n",
            "porter- quickli\n",
            "lancaster- quick\n",
            "snowball- quick\n",
            "regexp- quick\n",
            "\n",
            "token: \",\"\n",
            "porter- ,\n",
            "lancaster- ,\n",
            "snowball- ,\n",
            "regexp- ,\n",
            "\n",
            "token: \"and\"\n",
            "porter- and\n",
            "lancaster- and\n",
            "snowball- and\n",
            "regexp- and\n",
            "\n",
            "token: \"it\"\n",
            "porter- it\n",
            "lancaster- it\n",
            "snowball- it\n",
            "regexp- it\n",
            "\n",
            "token: \"made\"\n",
            "porter- made\n",
            "lancaster- mad\n",
            "snowball- made\n",
            "regexp- mad\n",
            "\n",
            "token: \"him\"\n",
            "porter- him\n",
            "lancaster- him\n",
            "snowball- him\n",
            "regexp- him\n",
            "\n",
            "token: \"realize\"\n",
            "porter- realiz\n",
            "lancaster- real\n",
            "snowball- realiz\n",
            "regexp- realiz\n",
            "\n",
            "token: \"that\"\n",
            "porter- that\n",
            "lancaster- that\n",
            "snowball- that\n",
            "regexp- that\n",
            "\n",
            "token: \"he\"\n",
            "porter- he\n",
            "lancaster- he\n",
            "snowball- he\n",
            "regexp- h\n",
            "\n",
            "token: \"could\"\n",
            "porter- could\n",
            "lancaster- could\n",
            "snowball- could\n",
            "regexp- could\n",
            "\n",
            "token: \"run\"\n",
            "porter- run\n",
            "lancaster- run\n",
            "snowball- run\n",
            "regexp- run\n",
            "\n",
            "token: \"even\"\n",
            "porter- even\n",
            "lancaster- ev\n",
            "snowball- even\n",
            "regexp- even\n",
            "\n",
            "token: \"faster\"\n",
            "porter- faster\n",
            "lancaster- fast\n",
            "snowball- faster\n",
            "regexp- fast\n",
            "\n",
            "token: \".\"\n",
            "porter- .\n",
            "lancaster- .\n",
            "snowball- .\n",
            "regexp- .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "- Lemmatization is more intelligent process that ensures resulting form is a valid word."
      ],
      "metadata": {
        "id": "oMVTVwEV82tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "U0HsN1iJ8w_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\") # english model"
      ],
      "metadata": {
        "id": "k5SJttgT9Yno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nlp(text)\n",
        "\n",
        "for token in tokens:\n",
        "  print(f'{token} | {token.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJOfcJCo9Zcv",
        "outputId": "81715c85-3e7f-4f86-d975-b7bcdb709f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The | the\n",
            "runner | runner\n",
            "was | be\n",
            "running | run\n",
            "quickly | quickly\n",
            ", | ,\n",
            "and | and\n",
            "it | it\n",
            "made | make\n",
            "him | he\n",
            "realize | realize\n",
            "that | that\n",
            "he | he\n",
            "could | could\n",
            "run | run\n",
            "even | even\n",
            "faster | fast\n",
            ". | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc to text conversion"
      ],
      "metadata": {
        "id": "QXNo-8GnEVbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtbckQiTEgxo",
        "outputId": "86100c42-b4c8-41ec-d55c-b19babb9cb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=19b6719bade2cd7a702170818e9a0ff8c95b2a0c86ac35dc1a0de909bc33abb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt as d2t"
      ],
      "metadata": {
        "id": "8cv4y7N69ZSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o_KBSUQEfVs",
        "outputId": "103e4935-d67a-466c-b5c5-c1122f1c2e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path = \"/content/drive/MyDrive/SPIDER/docs/Transformer.docx\"\n",
        "text_path = \"/content/Transformer.txt\"\n",
        "\n",
        "doc = d2t.process(doc_path)\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "YMIKu_68GS8T",
        "outputId": "faf238a8-d063-4bb6-de2b-068afb0a29c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tWenhao Li1\\tMengyuan Liu1,*\\tHong Liu1,*\\tPichao Wang2\\tJialun Cai1\\tNicu Sebe3 1National Key Laboratory of General Artificial Intelligence, Peking University, Shenzhen Graduate School 2Amazon Prime Video\\t3University of Trento\\n\\n\\t\\t\\t{wenhaoli,hongliu}@pku.edu.cn\\tnkliuyifang@gmail.com pichaowang@gmail.com\\tcjl@stu.pku.edu.cn\\tniculae.sebe@unitn.it\\n\\n\\n\\n\\n\\nAbstract\\n\\nTransformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained de- vices. In this paper, we present a plug-and-play pruning-and- recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of re- dundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer\\n\\n\\n\\n\\n\\n\\n\\n\\n250\\n\\n\\n\\n\\n\\nFLOPs (G)\\n\\nFLOPs (G)200\\n\\n\\n\\n150\\n\\n\\n\\n100\\n\\n\\n\\n50\\n\\n\\n\\n0\\n\\n\\n\\n\\n\\n\\nPoseFormer [54]\\n\\nStrided [21]\\n\\nMHFormer [20]\\n\\nP-STMO [36]\\n\\nMixSTE [51]\\n\\nMotionBERT   [55] TPC w. MixSTE (Ours) TPC w. MotionBERT (Ours) HoT w. MixSTE (Ours) HoT w. MotionBERT (Ours)\\n\\nPoseFormer [54]\\n\\nStrided [21]\\n\\nMHFormer [20]\\n\\nP-STMO [36]\\n\\nMixSTE [51]\\n\\nMotionBERT   [55] TPC w. MixSTE (Ours) TPC w. MotionBERT (Ours) HoT w. MixSTE (Ours) HoT w. MotionBERT (Ours)\\n\\n\\t\\t\\t\\t\\t39\\t40\\t41\\t42\\t43\\t44\\n\\nMPJPE (mm) ↓\\n\\n\\n\\n\\n\\narXiv:2311.12028v1\\n\\narXiv:2311.12028v1\\n\\n[cs.CV] 20 Nov 2023\\n\\n[cs.CV] 20 Nov 2023blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and estimation accuracy compared to the original VPT models. For instance, applying to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs without sacrificing accu- racy and nearly 40% FLOPs with only 0.2% accuracy drop, respectively. Our source code will be open-sourced.\\n\\n\\n\\n\\n\\n\\tIntroduction\\n\\n3D human pose estimation (HPE) from videos has numer- ous applications, such as action recognition [23, 26, 41], human-robot interaction [10, 56], and computer animation [31]. Current video-based 3D HPE methods mainly follow the pipeline of 2D-to-3D pose lifting [2, 12, 46, 47, 50]. This two-stage pipeline first utilizes an off-the-shelf 2D HPE\\n\\n\\n\\n*Corresponding Author.\\n\\n\\nFigure 1. FLOPs and estimation errors (MPJPE, lower is better) of different VPTs on Human3.6M dataset. We achieve highly competitive or even better results while saving FLOPs.\\n\\n\\n\\nmodel to detect 2D body joints for each video frame and then employs a separate lifting model to estimate 3D pose sequences from the detected 2D poses.\\n\\nRecently, transformer-based architectures [20, 51, 54, 55] have shown state-of-the-art (SOTA) performance in the field of video-based 3D HPE, since they are effective at modeling the long-range dependencies among video frames. These video pose transformers (VPTs) typically regard each video frame as a pose token and utilize extremely long video se- quences to achieve superior performance (e.g., 81 frames in [54], 243 frames in [36, 51, 55], or 351 frames in [8, 20, 21]). However, these methods inevitably suffer from high compu- tational demands since the VPT’s self-attention complexity grows quadratically with respect to the number of tokens (i.e., frames), hindering the deployment of these heavy VPTs on devices with limited computing resources.\\n\\nTo achieve efficient VPTs, two crucial factors require careful consideration: (i) Directly reducing the frame num- ber can improve the efficiency of VPTs, but it results in a small temporal receptive field that limits the model’s abil- ity to capture long-range temporal relationships [4, 24, 34]. Hence, it is important to design an efficient solution while maintaining a large temporal receptive field for accurate es-\\n\\n\\n\\n\\tInput Pose Tokens\\tInput Pose Tokens\\n\\n\\n\\n\\t\\t\\n\\nToken Pruning\\n\\nToken Recovering\\n\\nToken Pruning\\n\\nToken Recovering\\n\\n\\tOutput Pose Tokens\\tOutput Pose Tokens\\n\\n\\t(a) VPT\\t(b) HoT (Ours)\\n\\nFigure 2. (a) Existing VPTs follow a “rectangle” paradigm that retains the full-length sequence across all blocks, which incurs expensive and redundant computational costs. (b) Instead, our HoT follows an “hourglass” paradigm that prunes the pose tokens and recovers the full-length tokens, which keeps a few tokens in the intermediate transformer blocks and thus improves the model efficiency. The gray squares represent the pruned tokens.\\n\\n\\n\\ntimation. (ii) Adjacent frames in a video sequence contain redundant information due to the similarity of nearby poses (50 Hz cameras used in Human3.6M [13]). Moreover, recent studies [18, 35, 43] found that some tokens tend to be similar in the deep transformer blocks. Thus, we infer that using full-length pose tokens in these blocks leads to redundant calculations and contributes little to the final estimation.\\n\\nBased on these observations, we propose to prune pose to- kens in the deep transformer blocks to improve the efficiency of VPTs. Although token pruning can reduce the number of tokens and bring efficiency, it also makes it difficult to estimate the consecutive 3D pose of all frames, as in existing VPTs [20, 51, 55], where each token corresponds to a frame. Additionally, for efficient inference, a real-world 3D HPE system should be able to estimate the 3D poses of all frames at once in an input video. Therefore, in order to make our method more compatible with being plugged into existing VPTs and achieve fast inference, we need to recover the original full-length tokens for all-frame estimation.\\n\\nDriven by this analysis, we present a novel pruning-and- recovering framework for efficient transformer-based 3D HPE from videos. Different from existing VPTs that main- tain the full-length sequence across all blocks, our method begins with pruning the pose tokens of redundant frames and ends with recovering the full-length tokens. By using these two designs, we can keep only a few tokens in the intermediate transformer blocks and thus improve the model efficiency (see Figure 2). For this to be achieved effectively, we argue that the key is to select a few representative tokens with high semantic diversity, as such tokens can maintain rich information while reducing video redundancy. Since the cluster centers can retain the semantic diversity of the origi- nal signal, we propose a token pruning cluster (TPC) module\\n\\n\\nthat utilizes the cluster to dynamically select the cluster cen- ters as the representative tokens. Furthermore, we develop a lightweight token recovering attention (TRA) module to restore the detailed spatio-temporal information based on the selected tokens, which expands the low temporal resolution caused by pruning operation to the full temporal resolution. This strategy enables the network to estimate consecutive 3D poses of all frames, which facilitates fast inference.\\n\\nOur method can be easily integrated into existing VPTs [20, 51, 55] with minimal modifications (see Figure 3). Specifically, the first few transformer blocks of VPTs re- main unchanged to obtain pose tokens with comprehensive information from full video frames. These pose tokens are then pruned by our TPC, and the remaining tokens that serve as the representative tokens are further fed into the subse- quent transformer blocks. Finally, the full-length tokens are recovered by TRA, which is added after the last transformer block, while the intermediate transformer blocks still use representative tokens. Thus the additional parameters and FLOPs from TRA are negligible. Since the number of tokens first decreases through pruning and then increases through recovering, we refer to the framework as an hourglass and name it as Hourglass Tokenizer (HoT).\\n\\nTo validate the effectiveness and efficiency of our method, we deploy it on top of SOTA VPTs (MHFormer [20], MixSTE [51], and MotionBERT [55]). Extensive experi- ments demonstrate that existing VPTs consume huge un- necessary computational costs in capturing temporal infor- mation, and the proposed method can not only maintain the ability of the model but also reduce the computational costs. As shown in Figure 1, our HoT can reduce nearly 50% floating-point operations (FLOPs) on MotionBERT\\n\\n[55] without sacrificing performance and nearly 40% FLOPs on MixSTE [51] with only 0.2% performance loss.\\n\\nThe contributions of our paper are summarized below:\\n\\n\\t\\tWe present HoT, a plug-and-play pruning-and- recovering framework for efficient transformer-based 3D HPE from videos. Our HoT reveals that maintaining the full-length pose sequence is redundant, and a few pose tokens of representative frames can achieve both high efficiency and performance.\\n\\n\\t\\tTo accelerate VPTs effectively, we propose a TPC mod- ule to select a few representative tokens for video re- dundancy reduction and a TRA module to restore the original temporal resolution for fast inference.\\n\\n\\t\\t\\tExtensive experiments are performed on three recent VPTs to validate the effectiveness and efficiency of the proposed method. Our results show that HoT achieves highly competitive or even better results while bringing significant improvements in efficiency.\\n\\n\\n\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToken Pruning Cluster\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToken Pruning Cluster\\t \\t\\n\\n\\n\\n(F, J, 3)\\n\\nToken Recovering Attention\\n\\n3D Poses\\n\\n(F, J, 3)\\n\\nToken Recovering Attention\\n\\n3D Poses\\n\\n(F, J, 2)\\n\\n2D Poses\\n\\n(F, J, 2)\\n\\n2D Poses\\n\\nPose Embedding\\n\\nPose Embedding\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nRegression Head\\n\\nRegression HeadFigure 3. Overview of the proposed Hourglass Tokenizer (HoT). It mainly consists of a token pruning cluster (TPC) module and a token recovering attention (TRA) module. TPC selects the pose tokens of representative frames after the first few transformer blocks and TRA recovers the full-length tokens after the last transformer block.\\n\\n\\n\\n\\tRelated Work\\n\\nTransformer-based 3D HPE. Transformers are firstly pro- posed in [39] and have been successfully applied to video- based 3D HPE [20, 51, 54, 55]. These video pose transform- ers (VPTs) are often built to capture spatial and temporal information for 3D HPE using transformers. For instance, MHFormer [20] learns spatio-temporal multi-hypothesis rep- resentations of 3D human poses via transformers. MixSTE\\n\\n[51] proposes a mixed spatio-temporal transformer to capture the temporal motion of different body joints. MotionBERT\\n\\n[55] presents a dual-stream spatio-temporal transformer to model long-range spatio-temporal relationships among skele- tal joints. However, the improved performance of these VPTs comes with a heavy computation burden. In this work, we improve the efficiency of existing VPTs by keeping a few representative tokens in the intermediate transformer blocks.\\n\\nEfficient 3D HPE. Efficient 3D HPE is critical in computing resource-constrained environments. Existing explorations mainly focus on efficient architecture design [5, 29, 32, 34]\\n\\nand data redundancy reduction [8, 21, 36, 48]. VPose [34] presents a fully convolutional architecture that processes multiple frames in parallel. Strided [21] designs a strided transformer encoder to aggregate redundant sequences. Re- cently, several studies [8, 36, 48] have attempted to improve model efficiency by uniformly sampling video sequences. For example, DeciWatch [48] proposes a flow that takes sparsely sampled frames as inputs. However, this is sub- optimal as it simply selects frames at a fixed interval in a static manner without considering their contextual cues. In contrast, we propose to utilize the cluster to dynamically select pose tokens of representative frames with high-level semantic representations. Besides, many efficient methods [8, 21, 53] are designed for a specific model and some meth- ods [37, 48] are built on single-frame estimators [14–16, 29]. However, none of them unify the efficient design for differ- ent VPTs. We are the first to demonstrate a plug-and-play framework for efficient VPTs, which can be plugged into common VPT models.\\n\\n\\nToken Pruning for Transformers. The self-attention com- plexity in transformers grows quadratically with the number of tokens, making it infeasible for high spatial or temporal resolution inputs. Many works [3, 6, 17, 25, 44] attempt to alleviate this issue by using token pruning, which aims to select significant tokens from different inputs. They find that discarding less informative tokens in the deep transformer blocks only leads to a slight performance drop. Dynam- icViT [35] proposes a learnable prediction module to esti- mate the scores of tokens and prune redundant tokens. PPT\\n\\n[27] selects important tokens based on the attention score. TCFormer [49] presents a token clustering transformer to cluster and merge tokens. In this work, we are the first to perform token pruning in VPTs for model acceleration. Un- like these studies that aim to reduce less related information (e.g., image background) from images in the spatial domain, we focus on reducing temporal redundancy by selecting a few pose tokens of representative frames in the temporal domain. Furthermore, we propose to restore the full-length temporal resolution to meet the domain-specific requirement of efficient video-based 3D HPE.\\n\\n\\tMethod\\n\\n\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n\\t∈\\t≪\\n\\n\\t∈\\t≪Figure 3 illustrates the overview of our Hourglass Tok- enizer (HoT). Our HoT is a general-purpose pruning-and- recovering framework that can use different token pruning and token recovering strategies (see Sec 4.3). For better token pruning and recovering, we propose token pruning cluster (TPC) and token recovering attention (TRA) modules and insert them into SOTA VPTs [20, 51, 55]. Specifically, TPC takes the full-length pose tokens xn  RF ×J×C of n-th transformer block as inputs and outputs a few repre- sentative tokens x˜  Rf×J×C (f   F ), where J, F , and f are the number of body joints, input frames, and repre- sentative tokens, respectively. Here, C denotes the feature dimension. TRA recovers the full-length tokens from the tokens of the last transformer block xL  Rf×J×C, where\\n\\nL is the number of transformer blocks, resulting in recovered tokens xˆ ∈ RF ×J×C. In the following, we give the details\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSpatial Pooling\\n\\nClustering\\n\\nSpatial Pooling\\n\\nClustering\\n\\nLearnable Tokens\\n\\nQ\\n\\nK\\n\\nCross Attention\\n\\nV\\n\\nSum\\n\\nLearnable Tokens\\n\\nQ\\n\\nK\\n\\nCross Attention\\n\\nV\\n\\nSum\\n\\nPose Tokens\\n\\nPose Tokens\\n\\nRrepresentative Tokens\\n\\nRrepresentative Tokens\\n\\nRrepresentative Tokens\\n\\nRrepresentative Tokens\\n\\nRecovered Tokens\\n\\nRecovered TokensCluster Centers\\n\\n\\n\\n\\n\\nFigure 4. Illustration of our token pruning cluster (TPC) archi- tecture. Given the input pose tokens, we pool them in the spatial dimension, cluster the input tokens into several groups according to the feature similarity of the resulting pooled tokens, and select the cluster centers as the representative tokens.\\n\\n\\nFigure 5. Illustration of our token recovering attention (TRA) archi- tecture. TRA takes the representative tokens of the last transformer block, along with learnable tokens that are initialized to zero, as input to recover the full-length tokens.\\n\\nof each token is calculated by:\\n\\n\\n\\nabout the proposed TPC and TRA modules and show how\\n\\n\\n  minj:ρ >ρ\\n\\n\\n¨xi − xj¨\\n\\n\\n, if ∃ρj > ρi\\n\\n\\n\\nto apply them to existing VPTs.\\n\\n\\nδi =\\n\\n\\nj\\n\\n\\n\\n¨\\n\\n¨\\n\\ni\\n\\nimaxj ¨x\\n\\n\\ni\\n\\n— xj¨\\n\\n\\n2\\n\\n2 , otherwise\\n\\n\\t\\n.\\t(2)\\n\\n\\n\\n\\tToken Pruning Cluster\\n\\nWe observe that the existing VPTs [20, 51, 55] take long video sequences as input and maintain the full-length se- quence across all blocks (Figure 2 (a)), which is computa- tionally expensive for high temporal resolution inputs. To tackle this issue, we propose to prune the pose tokens of video frames to improve the efficiency of VPTs. However, it is challenging to select a few pose tokens that maintain rich information for accurate 3D HPE.\\n\\n\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈To address this challenge, we propose a simple, effective, and parameter-free token pruning cluster (TPC) that dynami- cally selects a few pose tokens of representative frames to eliminate video redundancy. The architecture of TPC is il- lustrated in Figure 4. Given the input pose tokens of n-th transformer blocks xn  RF ×J×C, an average spatial pool- ing is used along the spatial dimension to remove spatial redundancy, resulting in pooled tokens xn RF ×C. Then, we apply an efficient density peaks clustering based on k- nearest neighbors (DPC-kNN) algorithm [7]. This algorithm clusters the input pose tokens into several groups accord- ing to the feature similarity of the pooled tokens without requiring an iterative process.\\n\\n\\n\\nj  Σ i\\n\\nj  Σ i\\n\\n\\ti\\tj 2\\n\\n\\ti\\tj 2The cluster centers of tokens are characterized by a higher density compared to their neighbors, as well as a relatively large distance from other tokens with higher densities. For a token xi ∈ xn, the local density of tokens ρ is calculated by:\\n\\n\\nThe clustering center score of a token xi is denoted by combining the local density ρi and minimal distance δi as ρi  δi. A higher score indicates that the token has both a large density and distance, showing a higher potential to be the cluster center. The top-f -scored input pose tokens are selected as cluster centers, and the remaining tokens are assigned to the nearest cluster center with higher density.\\n\\n\\n\\n×\\n\\n×\\n\\n∈\\n\\n∈The cluster centers have high semantic diversity, con- taining more informative information than the other tokens. Therefore, the cluster centers serve as the representative tokens x˜ Rf×J×C for efficient estimation, and the remain- ing tokens are discarded for reduction of video redundancy. Note that our method only prunes the tokens along the tem- poral dimension since the frame number F is much larger than the joint number J (e.g., F =243 and J=17), i.e., the expensive and redundant computational costs are dominated by the frame number in the temporal domain.\\n\\n\\tToken Recovering Attention\\n\\nA large number of pose tokens have been pruned by TPC, which significantly reduces the computational costs. How- ever, for fast inference, a real-world 3D HPE system should be capable of estimating the consecutive 3D poses of all frames in a given video (this is called seq2seq pipeline in [51]). Therefore, different from some token pruning meth- ods in vision transformers that can use a few selected tokens to directly perform classification [22, 28, 35, 45], we need\\n\\n\\n\\n1\\n\\nρi = exp(− k\\n\\n\\n\\n\\n\\n\\nx ∈kNN(x )\\n\\n\\t\\n¨x − x ¨2),\\t(1)\\n\\n\\nto recover the full-length tokens to keep the same number of tokens as the input video frames (in existing VPTs, each token corresponds to a frame). Meanwhile, for efficiency\\n\\n\\n\\n\\n\\n     \\n\\n     where kNN xi are the k-nearest neighbors of a token xi. We then define the δi that measures the minimal distance between the token xi and other tokens with higher density. The δi of the token with the highest density is set to the maximum distance between it and any other tokens. The δi\\n\\n\\npurposes, the recovering module should be lightweight.\\n\\nTo this end, a lightweight token recovering attention (TRA) module is proposed to restore the spatio-temporal information from the selected pose tokens, as shown in Fig- ure 5. It only contains one multi-head cross-attention (MCA)\\n\\n\\n\\n\\n\\nToken Pruning Cluster\\n\\n2D Poses\\n\\n3D Pose of Center Frame\\n\\nToken Pruning Cluster\\n\\n2D Poses\\n\\n3D Pose of Center Frame\\n\\nPose Embedding\\n\\nPose Embedding\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nTransformer Blocks\\n\\nRegression Head\\n\\nRegression Headlayer without any additional networks. Formally, the dot- product attention [39] in the MCA is defined as:\\n\\n\\tAttention(Q, K, V ) = Softmax QKT /√d V,\\t(3) where queries Q ∈ Rnq ×d, keys K ∈ Rnk ×d, and values\\n\\n\\n\\nq\\n\\nq\\n\\nk\\n\\nk\\n\\nv\\n\\nvV ∈ Rnv ×d. d is the dimension and {n , n , n } are the\\n\\n\\n\\n\\n\\n\\t{\\t}\\n\\n\\t{\\t}number of tokens for Q, K, V , respectively.\\n\\n\\n\\n∈\\n\\n∈\\n\\nL\\n\\nL\\n\\n∈\\n\\n∈Our MCA takes the learnable tokens x′  RF ×C that are initialized to zero as queries and the j-th joint representative tokens of the last transformer block xj  Rf×C as keys and values, followed by a residual connection:\\n\\n\\t\\n\\n\\tL\\tL\\n\\n\\tL\\tLxˆj = x′ + MCA(x′, xj , xj ),\\t(4)\\n\\n\\nFigure 6. Illustration of our framework on seq2frame pipeline. The pose tokens are fed into TPC to select representative tokens. After the regression head, the 3D pose of the center frame is selected as the output for evaluation.\\n\\nrestores the original temporal resolution and produces re- covered tokens xˆ ∈ RF ×J×C. Finally, a regression head is\\n\\n\\n\\nadded to estimate the 3D pose sequence q ∈ RF ×J×3.\\n\\n\\n\\n\\n\\n∈\\n\\n∈\\n\\n·\\n\\n·where MCA( ) is the function of MCA, and its inputs are queries, keys, and values. xˆj RF ×C is the j-th joint recovered token, whose temporal dimension is the same as the queries (i.e., the designed learnable tokens).\\n\\nThe TRA performs a reverse operation of selecting repre- sentative tokens, which recovers tokens of full-length tempo- ral resolution from low ones using high-level spatio-temporal semantic information.\\n\\n\\tApplying to VPTs\\n\\nRecent studies of VPTs can be divided into two types of pipelines based on their inference outputs: seq2frame [20, 21, 36, 54] and seq2seq [51, 55] pipelines. The seq2frame pipeline outputs the 3D pose of the center frame and requires repeated inputs of 2D pose sequences with sig- nificant overlap to predict the 3D poses of all frames. This pipeline can achieve better performance by considering both past and future information, but it is not efficient due to re- peated calculations. In contrast, the seq2seq pipeline outputs 3D poses of all frames from the input 2D pose sequence at once, making it more efficient but leading to a degradation in performance. As a result, these two pipelines have their unique strengths, and we need to develop two strategies to better accommodate their different inference manners.\\n\\n\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈\\n\\n∈For the seq2seq pipeline, the outputs are all frames of the input video, and hence we need to restore the original tempo- ral resolution. TPC and TRA are inserted into VPTs, where TPC prunes the tokens after a few transformer blocks and TRA recovers the full-length tokens after the last transformer block, as shown in Figure 3. Specifically, given the input 2D pose sequence p RF ×J×2 detected by an off-the-shelf 2D HPE detector from a video, we first feed them into a pose embedding module to embed spatial and temporal in- formation of pose frames, resulting in tokens x RF ×J×C. The embedded tokens are then fed into a few transformer blocks. Next, the TPC selects a few representative tokens x˜  Rf×J×C, which are the inputs of subsequent trans- former blocks. After the last transformer block, the TRA\\n\\n\\nFor the seq2frame pipeline, the output is the 3D pose of the center frame. Therefore, TRA is unnecessary and we only insert TPC into VPTs. As shown in Figure 6, the early stages of both pipelines share the same workflow. But in TPC, the center token is included in selected tokens to make this pipeline work better. After the last transformer block, the tokens are directly sent to the regression head to perform re- gression and the 3D pose of center frame qcenter  R1×J×3 is selected as the final prediction.\\n\\n\\t\\n\\n∈\\n\\n∈Experiments\\n\\n\\tDatasets and Evaluation Metrics\\n\\nDatasets. We evaluate our method on two 3D HPE bench- mark datasets: Human3.6M [13] and MPI-INF-3DHP [30]. Human3.6M is the most widely used dataset for 3D HPE. It consists of 3.6 million video frames recorded by four RGB cameras at 50 Hz in an indoor environment. This dataset includes 11 actors performing 15 daily actions. Following [11, 52, 57], subjects S1, S5, S6, S7, S8 are used for training and subjects S9, S11 are used for testing. MPI-INF-3DHP is another popular 3D HPE dataset. This dataset contains 1.3 million frames collected in indoor and outdoor scenes. It is smaller than Human3.6M, but more challenging due to its diverse scenes, viewpoints, and motions.\\n\\nEvaluation Metrics. For Human3.6M, we use the most commonly used mean per joint position error (MPJPE) as the evaluation metric, which measures the average Euclidean distance between estimated and ground truth 3D joint co- ordinates in millimeters. For MPI-INF-3DHP, we follow previous works [20, 51, 54] to report metrics of MPJPE, percentage of correct keypoint (PCK) with the threshold of 150mm, and area under curve (AUC).\\n\\n\\tImplementation Details\\n\\nThe network is implemented using the PyTorch framework on one consumer-level NVIDIA RTX 3090 GPU with 24G memory. Our method builds upon MHFormer [20], MixSTE\\n\\n\\n\\nTable 1. Comparison of efficiency and accuracy between seq2seq (*) and seq2frame (†) inference pipelines. FPS was computed on a single GeForce RTX 3090 GPU.\\n\\n\\n\\n\\n\\nMethod\\n\\n\\tParam (M) FLOPs (G)\\tFPS\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51] (*)\\n\\n\\t33.78  277.25\\t10432\\n\\n40.9\\n\\nHoT w. MixSTE (*)\\n\\n35.00  167.52 (↓ 39.6%) 15770 (↑ 51.2%)\\n\\n41.0\\n\\n\\n\\n\\n\\nMixSTE [51] (†) TPC w. MixSTE (†)\\n\\n33.78  277.25\\n\\n43\\n\\n33.78  161.73 (↓ 41.7%) 68 (↑ 58.1%)\\n\\n40.7\\n\\n40.4\\n\\nMixSTE [51] (†) TPC w. MixSTE (†)\\n\\n33.78  277.25\\n\\n43\\n\\n33.78  161.73 (↓ 41.7%) 68 (↑ 58.1%)\\n\\n40.7\\n\\n40.4\\n\\nMotionBERT [55] (*)\\n\\n16.00\\n\\n131.09\\n\\n14638\\n\\n39.8\\n\\nHoT w. MotionBERT (*)\\n\\n16.35\\n\\n63.21 (↓ 51.8%)\\n\\n25526 (↑ 74.4%)\\n\\n39.8\\n\\n\\n\\nMotionBERT [55] (†)\\n\\n16.00\\n\\n131.09\\n\\n60\\n\\n39.5\\n\\nTPC w. MotionBERT (†)\\n\\n16.00\\n\\n61.04 (↓ 53.4%)\\n\\n109 (↑ 81.7%)\\n\\n39.2\\n\\n\\n\\n\\t\\t{\\t}\\t{\\n\\n\\t\\t{\\t}\\t{[51], and MotionBERT [55] for their largest frame number (i.e., F =351, 243, 243) models. In addition to training Mo- tionBERT with a batch size of 4, we directly adopt their optimal hyper-parameters, training strategies, and loss func- tions. For a speed-accuracy trade-off, by default, we set F =351, n=1, f =117  for MHFormer,  F =243, n=3,\\n\\n\\n\\n\\t\\t}\\t{\\t}\\n\\n\\t\\t}\\t{\\t}f =81 for MixSTE, and F =243, n=1, f =81 for Mo- tionBERT. Note that MHFormer is designed for seq2frame pipeline, so we only implement our TPC on it. MixSTE and MotionBERT are designed for seq2seq pipeline and can be implemented on both seq2frame (with TPC) and seq2seq (with HoT) pipelines.\\n\\n\\tAblation Study\\n\\nTo validate the effectiveness of our method, we conduct extensive ablation studies on Human3.6M dataset.\\n\\nInference Pipeline. In Table 1, we compare the efficiency and accuracy between different inference pipelines (men- tioned in Sec 3.3). We conduct experiments on MixSTE\\n\\n\\n\\n×\\n\\n×[51] and MotionBERT [55] because both are designed for seq2seq pipeline and can be evaluated on both seq2frame and seq2seq pipelines. As shown in the table, the seq2frame can achieve better estimation accuracy by taking advantage of past and future information but lower efficiency due to repeated computations, e.g., 40.7mm vs. 40.9mm and 43 FPS vs. 10432 FPS for MixSTE (about 243 lower). As our TPC is parameter-free and TRA is lightweight, our method with TPC introduces no additional parameters, and HoT\\n\\nw. MotionBERT only introduces additional 0.35M (2.2%) parameters, which can be neglected. Besides, our method can both reduce the computational costs and improve the inference speed of these two pipelines while maintaining or obtaining better performance.\\n\\nFor the seq2seq, our method can reduce the FLOPs of MixSTE and MotionBERT by 39.6% and 51.8% and im- prove the FPS by 51.2% and 74.4%, while estimation errors only drop 0.1 mm (0.24%) and remain unchanged, respec- tively. For the seq2frame, our TPC w. MixSTE can reduce the FLOPs by 41.7% and improve the FPS by 58.1%, while bringing 0.3mm improvement. Additionally, our TPC w. MotionBERT can reduce 53.4% FLOPs and improve 81.7%\\n\\n\\nTable 2. Ablation study on the block index of representative tokens\\n\\n(n) under the seq2frame pipeline. Here, ∗ denotes the result without re-training.\\n\\n\\n\\n\\n\\nMethod\\n\\nParam (M) FLOPs (G)\\n\\nMPJPE∗ MPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n\\t33.78\\t277.25\\n\\n\\t40.7\\t40.7\\n\\n\\n\\nTPC w. MixSTE, n=2\\n\\n33.78\\n\\n121.52 (↓ 56.2%)\\n\\n41.2\\n\\n40.7\\n\\nTPC w. MixSTE, n=3\\n\\n33.78\\n\\n147.47 (↓ 46.8%)\\n\\n41.2\\n\\n40.5\\n\\nTPC w. MixSTE, n=5\\n\\n33.78\\n\\n199.38 (↓ 28.1%)\\n\\n40.9\\n\\n40.2\\n\\nTPC w. MixSTE, n=7\\n\\n33.78\\n\\n251.29 (↓ 09.4%)\\n\\n40.7\\n\\n39.9\\n\\nTable 3. Ablation study on the number of representative tokens (f ) under the seq2seq pipeline.\\n\\n\\n\\n\\n\\nMethod\\n\\nParam (M)  FLOPs (G)\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n\\t33.78\\t277.25\\n\\n40.9\\n\\n\\n\\nHoT w. MixSTE, f =9\\n\\n34.96\\n\\n114.90 (↓ 58.6%)\\n\\n43.5\\n\\nHoT w. MixSTE, f =16\\n\\n34.97\\n\\n120.01 (↓ 56.7%)\\n\\n42.2\\n\\nHoT w. MixSTE, f =61\\n\\n34.99\\n\\n152.90 (↓ 44.9%)\\n\\n41.2\\n\\nHoT w. MixSTE, f =81\\n\\n35.00\\n\\n167.52 (↓ 39.6%)\\n\\n41.0\\n\\nHoT w. MixSTE, f =135\\n\\n35.03\\n\\n206.98 (↓ 25.3%)\\n\\n41.3\\n\\n\\n\\nFPS, while the estimation errors are reduced from 39.5 mm to 39.2 mm. Note that our method with TPC outperforms the one utilizing HoT. This is reasonable since our TRA in HoT is a reverse operation that uses inadequate information to recover the full-length tokens. In the following ablations, we take these two inference pipelines into account to sufficiently explore the proposed method, and we choose MixSTE [51] as the baseline since it is the first seq2seq transformer-based architecture and MotionBERT [55] is its follow-ups.\\n\\nBlock index of Representative Tokens. The TPC can be inserted into optional transformer blocks, thereby adjusting the trade-off between computational costs and performance on demand in a flexible manner. Table 2 studies this under seq2frame pipeline (f is fixed to 61). Since TPC is a data- dependent scheme that introduces no extra parameters and transformers are input agnostic [1, 9], we can evaluate mod- els with or without re-training. Increasing the block index of representative tokens can reduce the estimation error, but it also leads to higher computational costs. This indicates the deeper blocks of transformers contain more redundancy while the shallower blocks retain more useful information. Our method achieves competitive results without re-training while reducing FLOPs. When it works with re-training (train- ing from scratch without pre-trained models), our method attains better performance. Our TPC w. MixSTE (n=2) achieves the same results while reducing 56.2% FLOPs and TPC w. MixSTE (n=7) improves the performance from 40.7mm to 39.9mm while reducing 9.4% FLOPs.\\n\\nNumber of Representative Tokens. The number of repre- sentative tokens f can also be flexibly adjusted. In Table 3, we fix n to 3 and vary f under seq2seq pipeline. Increasing f can reduce the FLOPs, but the best performance is achieved by using f =81. The reason for this is that an appropriate number of representative tokens can bring a good trade-off\\n\\n\\n\\n   \\n\\n\\n\\n\\t\\t\\t(a) Uniform Sampling\\t(b) Attention Score Pruning\\t(c) Motion Pruning\\t(d) The Proposed TPC\\n\\nFigure 7. Statistics visualization of selected tokens for different token pruning strategies. Top: Frame indexes of selected tokens for some samples (140 samples) of consecutive video sequences (243 frames). Blue points are selected tokens and white points are pruned tokens. Bottom: Frequency count of frame indexes of selected tokens for these samples.\\n\\n\\n\\nTable 4. Ablation study on the design choices of token pruning. “FN” denotes the frame noise that calculates the MPJPE of selected frames. “Full”, “Pruned”, “Selected”, and “Center” denote the MPJPE of all frames, pruned frames, selected frames, and the center frame, respectively.\\n\\n\\n\\n\\n\\nMethod\\n\\n\\n\\nFN\\n\\n\\tseq2seq\\tseq2frame\\n\\n\\n\\n\\n\\nFull ↓\\n\\nPruned\\n\\n↓\\n\\nSelected\\n\\n↓\\n\\nCenter\\n\\n↓\\n\\nSelected ↓\\n\\nMixSTE [51]\\n\\n6.61\\n\\n40.9\\n\\n-\\n\\n-\\n\\n40.7\\n\\n-\\n\\nOurs, Uniform Sampling [8, 36, 48]\\n\\n6.61\\n\\n41.4\\n\\n41.3\\n\\n41.4\\n\\n40.7\\n\\n40.8\\n\\nOurs, Attention Pruning [27, 42]\\n\\n6.56\\n\\n42.1\\n\\n42.5\\n\\n41.5\\n\\n42.3\\n\\n44.4\\n\\nOurs, Motion Pruning\\n\\n7.00\\n\\n42.8\\n\\n43.4\\n\\n41.6\\n\\n41.3\\n\\n42.3\\n\\nOurs, the Proposed TPC\\n\\n6.63\\n\\n41.0\\n\\n41.3\\n\\n40.2\\n\\n40.4\\n\\n39.4\\n\\n\\n\\nbetween retaining important information and reducing re- dundant information for both the pruning and recovering stages. Therefore, the optimal hyper-parameters for our HoT\\n\\nw. MixSTE are n=3 and f =81.\\n\\nToken Pruning Design. Our HoT is a general-purpose pruning-and-recovering framework that can be equipped with different token pruning and recovering strategies. In Ta- ble 4, we compare different token pruning strategies, includ- ing attention pruning [27, 42], uniform sampling [8, 36, 48], and motion pruning that selects tokens with top-k-large mo- tions. To measure the quality of selected tokens, we define the frame noise metric, which calculates the MPJPE of the input 2D frames corresponding to the selected indexes. As shown in the table, the frame noise values among these methods are similar (around 6.6mm) except for the motion pruning (7.0mm). This is because selecting tokens with top-k-large motion introduces some noise frames that dif- fer significantly from clean frames, which can adversely affect performance. Moreover, it is shown that the proposed TPC, which utilizes clustering to select representative tokens with high semantic diversity, outperforms all other token pruning strategies, particularly for selected frames. Our TPC outperforms the uniform sampling strategy by 1.2mm (40.2mm vs. 41.4mm) and 1.4mm (39.4mm vs. 40.8mm) for selected frames under the seq2seq and seq2frame pipeline, respectively. This emphasizes that the 3D pose results of our\\n\\n\\nTable 5. Ablation study on the design choices of token recovering.\\n\\n∆ represents the performance gap between the results of pruned frames and selected frames.\\n\\n\\n\\nMethod\\n\\nParam\\n\\nFLOPs\\n\\nFull ↓ Pruned\\n\\n↓\\n\\nSelected ↓\\n\\n∆\\n\\nMixSTE [51]\\n\\n33.78\\n\\n277.25\\n\\n\\t40.9\\t-\\n\\n-\\n\\n-\\n\\nOurs, Nearest Interpolation\\n\\n33.78\\n\\n161.73\\n\\n\\t41.5\\t42.2\\n\\n40.2\\n\\n2.0\\n\\nOurs, Linear Interpolation\\n\\n33.78\\n\\n161.73\\n\\n\\t41.3\\t41.9\\n\\n40.0\\n\\n1.9\\n\\nOurs, the Proposed TRA\\n\\n35.00\\n\\n167.52\\n\\n\\t41.0\\t41.3\\n\\n40.2\\n\\n1.1\\n\\n\\n\\nselected frames are easier to estimate, and our method can select more representative frames from a video.\\n\\nFurthermore, we statistically visualize selected tokens of these four token pruning strategies. For better observation, we take samples of consecutive video sequences as input with a temporal interval of 1 between neighboring samples. The frame indexes and the frequency count of frame indexes of the selected tokens are shown in Figure 7 (top) and Fig- ure 7 (bottom). The uniform sampling and motion pruning are static pruning methods because the former selects tokens at a fixed frame interval (equidistance in the top of Figure 7 (a)), while the latter selects tokens with the top-k-large mo- tions that move with the input sequence (oblique triangle in the top of Figure 7 (c)). Instead, the attention score pruning and our method are dynamic methods that consider the signif- icance of input tokens. The bottom of Figure 7 (b) shows that the selected tokens often appear in the left half of a sequence, demonstrating the selected tokens of attention score pruning tend to be similar to each other [43] and thus lacks diver- sity. Our method primarily selects tokens at the beginning, center, and end of a sequence (the bottom of Figure 7 (d)). This is reasonable since these three parts can represent the rough motion of an entire sequence, which contributes a lot to accurate estimation. Interestingly, the visualization figures of frame indexes between TPC and motion pruning are somewhat similar, indicating that our TPC also considers motion information. But the performance of motion prun- ing is much worse than our TPC due to noise frames (see\\n\\n\\n\\nTable 6. Comparison of parameters (M), FLOPs (G), and MPJPE with SOTA VPTs on Human3.6M. Here, F denotes the number of input frames. ∗ indicates our re-implementation.\\n\\n\\n\\n\\n\\nMethod\\n\\nF Param FLOPs\\n\\nMPJPE ↓\\n\\nMethod\\n\\nF Param FLOPs\\n\\nMPJPE ↓\\n\\n\\n\\nPoseFormer (ICCV’21) [54]\\n\\nStrided (TMM’22) [21]\\n\\n81\\n\\n351\\n\\n9.60\\n\\n4.35\\n\\n1.63\\n\\n1.60\\n\\n44.3\\n\\n43.7\\n\\nP-STMO (ECCV’22) [36]\\n\\n243\\n\\n7.01\\n\\n1.74\\n\\n42.8\\n\\nSTCFormer (CVPR’23) [38]\\n\\n243\\n\\n18.93\\n\\n156.22\\n\\n40.5\\n\\nMHFormer (CVPR’22) [20]\\n\\n351\\n\\n31.52\\n\\n14.15\\n\\n43.0\\n\\nTPC w. MHFormer (Ours)\\n\\n351\\n\\n31.52\\n\\n8.22 (↓ 41.91%)\\n\\n43.0\\n\\nMixSTE (CVPR’22) [51]\\n\\n243\\n\\n33.78\\n\\n277.25\\n\\n40.9\\n\\nHoT w. MixSTE (Ours)\\n\\n243\\n\\n35.00\\n\\n167.52 (↓ 39.6%)\\n\\n41.0\\n\\nTPC w. MixSTE (Ours)\\n\\n243\\n\\n33.78\\n\\n251.29 (↓ 9.4%)\\n\\n39.9\\n\\nMotionBERT (ICCV’23) [55]\\n\\n243\\n\\n16.00\\n\\n131.09\\n\\n39.2\\n\\nMotionBERT (ICCV’23) [55]∗\\n\\nHoT w. MotionBERT (Ours) TPC w. MotionBERT (Ours)\\n\\n\\t243 16.00 131.09\\t39.8\\n\\n\\t243 16.35 63.21 (↓ 51.8%)\\t39.8\\n\\n\\t243 16.00 91.38 (↓ 30.3%)\\t39.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable 4). These findings highlight that our method not only eliminates the redundancy of video frames but also selects tokens with high semantic diversity (the top of Figure 7 (d) appears to be irregular), thus selecting more representative pose tokens of video frames for more accurate estimation. Token Recovering Design. The token recovering strategies in our HoT can also be designed in different manners, as studied in Table 5. It shows that linear and nearest interpola- tion operations are parameter-free and achieve competitive results due to data redundancy (i.e., nearby poses are similar) on Human3.6M (captured by 50 Hz cameras). Our TRA achieves better performance while introducing negligible parameters and FLOPs. These results validate the effec- tiveness of the proposed TRA, highlighting the benefits of using high-level semantic information for pose token recov- ering. Besides, the experiments show that the proposed TRA achieves the lowest performance gap between the estimated 3D poses of pruned frames and selected frames. This further demonstrates the effectiveness of our TRA, which can re- cover more accurate results based on the limited information provided by the selected tokens.\\n\\n\\n\\n\\tComparison with state-of-the-art methods\\n\\n\\n\\n\\t{\\t}\\n\\n\\t{\\t}Human3.6M. Current SOTA performance on Human3.6M is achieved by transformer-based architectures. We com- pare our method with them by adding it to three very recent VPTs: MHFormer [20], MixSTE [51], and MotionBERT [55]. These three models significantly outperform previous works at the cost of high computational complexity, thus we choose them as baselines to evaluate our method. The comparisons are shown in Figure 1 and Table 6. We report the results of TPC w. MixSTE with n=7, f =61 and TPC\\n\\n\\n\\n\\t{\\t}\\n\\n\\t{\\t}w. MotionBERT with n=2, f =121 . Note that Motion-\\n\\nBERT [55] uses a batch size of 32 to train the model on 8 NVIDIA V100 GPUs (32G memory per GPU), which is too expensive and is not suitable for the real-world scenarios\\n\\n\\nTable 7. Quantitative comparison with SOTA methods on MPI- INF-3DHP.\\n\\n\\n\\nMethod\\n\\nPCK ↑\\n\\nAUC ↑\\n\\nMPJPE ↓\\n\\nVPose (CVPR’19) [34] (F =81)\\n\\n86.0\\n\\n51.9\\n\\n84.0\\n\\nUGCN (ECCV’20) [40] (F =96)\\n\\n86.9\\n\\n62.1\\n\\n68.1\\n\\nAnatomy3D (TCSVT’21) [4] (F =81)\\n\\n87.9\\n\\n54.0\\n\\n78.8\\n\\nPoseFormer (ICCV’21) [54] (F =9)\\n\\n88.6\\n\\n56.4\\n\\n77.1\\n\\nMHFormer (CVPR’22) [20] (F =9)\\n\\n93.8\\n\\n63.3\\n\\n58.0\\n\\nTPC w. MHFormer (Ours, F =9)\\n\\n94.0\\n\\n63.3\\n\\n58.4\\n\\nMixSTE (CVPR’22) [51] (F =27)\\n\\n94.4\\n\\n66.5\\n\\n54.9\\n\\nHoT w. MixSTE (Ours, F =27)\\n\\n94.8\\n\\n66.5\\n\\n53.2\\n\\nwe focus on. So we report the result of MotionBERT train- ing on a single GPU with a smaller batch size (see Sec. C). As shown in the table, our method can reduce the computa- tional costs of recent VPTs while maintaining the ability of the model. For example, our HoT w. MotionBERT saves 51.8% FLOPs while maintaining accuracy, and our TPC w. MotionBERT obtains better performance with 0.8mm im- provements while reducing computational costs by 30.3% in FLOPs. These results demonstrate the effectiveness and efficiency of our method, while also revealing that existing VPTs incur redundant computational costs that contribute little to the estimation accuracy or even decrease the accu- racy. In addition, our method can remove these unnecessary computational costs while achieving comparable or even superior performance.\\n\\n\\n\\n}\\n\\n}\\n\\n\\t\\t{\\t}\\t{\\n\\n\\t\\t{\\t}\\t{MPI-INF-3DHP. We further evaluate our method on MPI- INF-3DHP dataset in Table 7. For a fair comparison, fol- lowing [20, 51], we implement our method on MHFormer with  F =9, n=1, f =3  and MixSTE with  F =27, n=3, f =9 . It can be found that our method (TPC w. MHFormer and HoT w. MixSTE) achieves competitive performance, demonstrating the effectiveness of our method in both indoor and outdoor scenes. Besides, our method can also work well with a small temporal receptive field. Note that MotionBERT\\n\\n[55] does not report results on MPI-INF-3DHP, and thus we do not implement our method on it.\\n\\n\\n\\n\\tConclusion\\n\\nThis paper presents Hourglass Tokenizer (HoT), a plug- and-play pruning-and-recovering framework for efficient transformer-based 3D human pose estimation from videos. Our method reveals that maintaining the full pose sequence is unnecessary, and using a few pose tokens of representa- tive frames can achieve both high efficiency and estimation accuracy. Comprehensive experiments demonstrate that our method is compatible and general, capable of being easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating various token pruning and recovery strategies, thereby high- lighting its potential for using future ones. We hope HoT can enable the creation of stronger and faster VPTs.\\n\\n\\n\\nReferences\\n\\n\\t\\t\\tDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In ICLR, 2022.\\n\\n\\t\\t\\tYujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann. Exploit- ing spatial-temporal relationships for 3D pose estimation via graph convolutional networks. In ICCV, pages 2272–2281, 2019.\\n\\n\\t\\t\\tShuning Chang, Pichao Wang, Ming Lin, Fan Wang, David Junhao Zhang, Rong Jin, and Mike Zheng Shou. Mak- ing vision transformers efficient from a token sparsification view. In CVPR, pages 6195–6205, 2023.\\n\\n\\tTianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-aware 3D human pose esti- mation with bone-based pose decomposition. IEEE TCSVT, 32(1):198–209, 2021.\\n\\n\\tSangbum Choi, Seokeon Choi, and Changick Kim. Mobile- HumanPose: Toward real-time 3D human pose estimation in mobile devices. In CVPR, pages 2328–2338, 2021.\\n\\n\\t\\t\\tZhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao, Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping Wang. TORE: Token reduction for efficient human mesh recovery with transformer. In ICCV, pages 15143–15155, 2023.\\n\\n\\tMingjing Du, Shifei Ding, and Hongjie Jia. Study on density peaks clustering based on k-nearest neighbors and principal component analysis. Knowledge-Based Systems, 99:135–145, 2016.\\n\\n\\tMoritz Einfalt, Katja Ludwig, and Rainer Lienhart. Uplift and upsample: Efficient 3d human pose estimation with uplifting transformers. In WACV, pages 2903–2913, 2023.\\n\\n\\t\\t\\tMohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Ju¨ rgen Gall. Adaptive token sampling for efficient vision transformers. In ECCV, pages 396–414, 2022.\\n\\n\\t\\t\\tMercedes Garcia-Salguero, Javier Gonzalez-Jimenez, and Francisco-Angel Moreno. Human 3D pose estimation with a tilting camera for social mobile robot interaction. Sensors, 19 (22):4943, 2019.\\n\\n\\t\\tKehong Gong, Jianfeng Zhang, and Jiashi Feng. PoseAug: A differentiable pose augmentation framework for 3D human pose estimation. In CVPR, pages 8575–8584, 2021.\\n\\n\\t\\tWenbo Hu, Changgong Zhang, Fangneng Zhan, Lei Zhang, and Tien-Tsin Wong. Conditional directed graph convolution for 3D human pose estimation. In ACMMM, pages 602–611, 2021.\\n\\n\\tCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predic- tive methods for 3D human sensing in natural environments. IEEE TPAMI, 36(7):1325–1339, 2013.\\n\\n\\t\\tHanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex- emplar fine-tuning for 3D human model fitting towards in- the-wild 3D human pose estimation. In 3DV, pages 42–52, 2021.\\n\\n\\t\\t\\nMuhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges, and Michael J Black. PARE: Part attention regressor for 3D human body estimation. In ICCV, pages 11127–11137, 2021.\\n\\n\\t\\tNikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3D human pose and shape via model-fitting in the loop. In ICCV, pages 2252–2261, 2019.\\n\\n\\t\\t\\tZhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. SPViT: Enabling faster vision transformers via latency-aware soft token pruning. In ECCV, pages 620–640, 2022.\\n\\n\\t\\t\\tLing Li, David Thorsley, and Joseph Hassoun. Sait: Sparse vision transformers through adaptive token pruning. arXiv preprint arXiv:2210.05832, 2022.\\n\\n\\t\\t\\tShichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, and Kwang-Ting Cheng. Cascaded deep monocular 3D human pose estimation with evolutionary training data. In CVPR, pages 6173–6183, 2020.\\n\\n\\t\\t\\tWenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool. MHFormer: Multi-hypothesis transformer for 3D human pose estimation. In CVPR, pages 13147–13156, 2022.\\n\\n\\t\\t\\tWenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang. Exploiting temporal contexts with strided transformer for 3D human pose estimation. IEEE TMM, 25:1282–1293, 2023.\\n\\n\\t\\t\\tYouwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. In ICLR, 2022.\\n\\n\\t\\t\\tMengyuan Liu, Hong Liu, and Chen Chen. Enhanced skeleton visualization for view invariant human action recognition. PR, 68:346–362, 2017.\\n\\n\\t\\tRuixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, and Vijayan Asari. Attention mechanism exploits temporal contexts: Real-time 3D human pose reconstruction. In CVPR, pages 5064–5073, 2020.\\n\\n\\t\\t\\tSifan Long, Zhen Zhao, Jimin Pi, Shengsheng Wang, and Jingdong Wang. Beyond attentive tokens: Incorporating token importance and diversity for efficient vision transformers. In CVPR, pages 10334–10343, 2023.\\n\\n\\t\\tDiogo C Luvizon, David Picard, and Hedi Tabia. Multi-task deep learning for real-time 3D human pose estimation and action recognition. IEEE TPAMI, 43(8):2752–2764, 2020.\\n\\n\\t\\t\\tHaoyu Ma, Zhe Wang, Yifei Chen, Deying Kong, Liangjian Chen, Xingwei Liu, Xiangyi Yan, Hao Tang, and Xiaohui Xie. PPT: Token-pruned pose transformer for monocular and multi-view human pose estimation. In ECCV, pages 424–442, 2022.\\n\\n\\t\\tDmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token pool- ing in vision transformers. arXiv preprint arXiv:2110.03860, 2021.\\n\\n\\t\\tJulieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3D human pose estimation. In ICCV, pages 2640–2649, 2017.\\n\\n\\n\\n\\t\\tDushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Olek- sandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3D human pose estimation in the wild using im- proved CNN supervision. In 3DV, pages 506–516, 2017.\\n\\n\\t\\t\\tDushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt. VNect: Real-time 3D human pose estimation with a single rgb cam- era. ACM TOG, 36(4):1–14, 2017.\\n\\n\\t\\t\\tDushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed Elgharib, Pascal Fua, Hans-Peter Sei- del, Helge Rhodin, Gerard Pons-Moll, and Christian Theobalt. XNect: Real-time multi-person 3D motion capture with a sin- gle RGB camera. ACM TOG, 39(4):82–1, 2020.\\n\\n\\t\\t\\tAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour- glass networks for human pose estimation. In ECCV, pages 483–499, 2016.\\n\\n\\t\\tDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3D human pose estimation in video with tem- poral convolutions and semi-supervised training. In CVPR, pages 7753–7762, 2019.\\n\\n\\t\\tYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision transformers with dynamic token sparsification. In NeurIPS, pages 13937–13949, 2021.\\n\\n\\t\\t\\tWenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. P-STMO: Pre-trained spatial tem- poral many-to-one model for 3D human pose estimation. In ECCV, 2022.\\n\\n\\t\\t\\tYuran Sun, Alan William Dougherty, Zhuoying Zhang, Yi King Choi, and Chuan Wu. MixSynthFormer: A trans- former encoder-like structure with mixed synthetic self- attention for efficient human pose estimation. In ICCV, pages 14884–14893, 2023.\\n\\n\\t\\t\\tZhenhua Tang, Zhaofan Qiu, Yanbin Hao, Richang Hong, and Ting Yao. 3D human pose estimation with spatio-temporal criss-cross attention. In CVPR, pages 4790–4799, 2023.\\n\\n\\t\\tAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008, 2017.\\n\\n\\t\\tJingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin. Motion guided 3D pose estimation from videos. In ECCV, pages 764–780, 2020.\\n\\n\\t\\t\\tPichao Wang, Wanqing Li, Zhimin Gao, Chang Tang, and Philip O Ogunbona. Depth pooling based large-scale 3D action recognition with convolutional neural networks. IEEE TMM, 20(5):1051–1061, 2018.\\n\\n\\t\\tPichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning Chang, Hao Li, and Rong Jin. KVT: K-nn attention for boosting vision transformers. In ECCV, pages 285–302, 2022.\\n\\n\\t\\tZhenyu Wang, Hao Luo, Pichao Wang, Feng Ding, Fan Wang, and Hao Li. VTC-LFC: Vision transformer compression with low-frequency components. In NeurIPS, pages 13974–13988, 2022.\\n\\n\\t\\tYutong Xie, Jianpeng Zhang, Yong Xia, Anton van den Hengel, and Qi Wu.  ClusTR: Exploring efficient self-\\n\\n\\nattention via clustering for vision transformers. arXiv preprint arXiv:2208.13138, 2022.\\n\\n\\t\\t\\tHongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-ViT: Adaptive tokens for efficient vision transformer. In CVPR, pages 10809–10818, 2022.\\n\\n\\t\\t\\tAiling Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu, and Stephen Lin. SRNet: Improving generalization in 3D human pose estimation with a split-and-recombine approach. In ECCV, pages 507–523, 2020.\\n\\n\\t\\tAiling Zeng, Xiao Sun, Lei Yang, Nanxuan Zhao, Minhao Liu, and Qiang Xu. Learning skeletal graph neural networks for hard 3D pose estimation. In ICCV, pages 11436–11445, 2021.\\n\\n\\t\\t\\tAiling Zeng, Xuan Ju, Lei Yang, Ruiyuan Gao, Xizhou Zhu, Bo Dai, and Qiang Xu. DeciWatch: A simple baseline for 10x efficient 2D and 3D pose estimation. In ECCV, pages 607–624, 2022.\\n\\n\\t\\t\\tWang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang. Not all tokens are equal: Human-centric visual analysis via token clustering transformer. In CVPR, pages 11101–11111, 2022.\\n\\n\\t\\t\\tJinlu Zhang, Yujin Chen, and Zhigang Tu. Uncertainty- aware 3D human pose estimation from monocular video. In ACMMM, pages 5102–5113, 2022.\\n\\n\\t\\t\\tJinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Jun- song Yuan. MixSTE: Seq2seq mixed spatio-temporal encoder for 3D human pose estimation in video. In CVPR, pages 13232–13242, 2022.\\n\\n\\t\\tLong Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dim- itris N Metaxas. Semantic graph convolutional networks for 3D human pose regression. In CVPR, pages 3425–3435, 2019.\\n\\n\\t\\tQitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen Chen. PoseFormerV2: Exploring frequency domain for efficient and robust 3D human pose estimation. In CVPR, pages 8877–8886, 2023.\\n\\n\\t\\t\\tCe Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3D human pose estimation with spatial and temporal transformers. In ICCV, pages 11656– 11665, 2021.\\n\\n\\t\\t\\tWentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, and Yizhou Wang. MotionBERT: A unified perspective on learning human motion representations. In ICCV, pages 15085–15099, 2023.\\n\\n\\t\\tChristian Zimmermann, Tim Welschehold, Christian Dorn- hege, Wolfram Burgard, and Thomas Brox. 3D human pose estimation in rgbd images for robotic task learning. In ICRA, pages 1986–1992, 2018.\\n\\n\\t\\t\\tZhiming Zou and Wei Tang. Modulated graph convolutional network for 3D human pose estimation. In ICCV, pages 11477–11487, 2021.\\n\\n\\n\\nSupplementary Material\\n\\nThis supplementary material covers the following details:\\n\\n\\tA brief description of video pose transformers (Sec. A).\\n\\n\\tComputation complexity of transformers (Sec. B).\\n\\n\\tAdditional implementation details (Sec. C).\\n\\n\\tAdditional quantitative results (Sec. D).\\n\\n\\tAdditional ablation studies (Sec. E)\\n\\n\\tAdditional visualization results (Sec. F).\\n\\n\\tVideo Pose Transformers\\n\\nRecent studies of video pose transformers (VPTs) [20, 21, 36, 51, 54, 55] are mainly designed to estimate 3D poses from 2D pose sequences. These VPTs share a similar ar- chitecture, which includes a pose embedding module to embed spatial and temporal information of pose sequences, a stack of transformer blocks to learn global spatio-temporal correlations, and a regression module to predict 3D hu- man poses. We summarize the architecture in Figure 8. There are two types of pipelines based on their outputs: the seq2frame pipeline outputs the 3D poses of all frames, while the seq2seq pipeline outputs the 3D pose of the center frame.\\n\\n\\tComputation Complexity\\n\\n\\t\\t\\n\\n\\tO\\t \\n\\n\\tO\\t \\n\\n\\t\\t\\tO\\t\\tO\\t \\n\\n\\t\\t\\tO\\t\\tO\\t Each transformer block consists of a multi-head self- attention (MSA) layer and a feed-forward network (FFN) layer. Let N be the number of tokens, D be the dimension, and 2D be the expanding dimension in the FFN (the expand- ing ratio in VPTs is typically 2). The calculational costs of MSA and FFN are\\t4ND2 + 2N 2D , and\\t4ND2 , respectively. Thus, the total computational complexity is 8ND2 + 2N 2D , which makes VPTs computationally expensive. Since the dimension D is important to determine the modeling ability and most recent VPTs employ a D of 512 or 256, we follow their hyperparameter settings and propose to prune pose tokens of video frames (i.e., reducing\\n\\nN ) to reduce the computational cost of VPTs.\\n\\n\\tAdditional Implementation Details\\n\\nOur method is built upon three very recent VPTs: MHFormer [20], MixSTE [51], and MotionBERT [55]. These VPTs achieve state-of-the-art performance but are computationally expensive compared to previous methods (see Table 6). We choose these VPTs as baselines to evaluate our method, which focuses on preserving the ability to model spatio- temporal dependencies while reducing computational costs. We adopt the optimal hyperparameters and training strategies used in [20, 51, 55], as shown in Table 8. We also use the same loss functions for training, such as MPJPE loss for MHFormer, and weighted MPJPE loss, temporal consistency loss (TCLoss), and mean per-joint velocity error (MPJVE) for MixSTE.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeq2seq\\n\\nSeq2seqŏ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3D Pose Sequence\\n\\n3D Pose of Center Frame\\n\\n3D Pose Sequence\\n\\n3D Pose of Center Frame\\n\\n2D Pose Sequence\\n\\n2D Pose Sequence\\n\\nPose Embedding\\n\\nPose Embedding\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nTransformer Block\\n\\nRegression Head\\n\\nRegression Head\\n\\nSeq2frame\\n\\nSeq2frameFigure 8. Summary of the VPT architecture. Existing VPTs typ- ically contain a pose embedding module, a stack of transformer blocks, and a regression module. The output of the regression head can be either the 3D poses of all frames for the seq2seq pipeline or the 3D pose of the center frame for the seq2frame pipeline.\\n\\nTable 8. Implementation details of our method on MHFormer [20], MixSTE [51], and MotionBERT [55]. (L) - number of transformer blocks; (C) - dimension. (LR) - initial learning rate. (Flip) - horizontal flip augmentation (CPN) - Cascaded Pyramid Network\\n\\n[19]; (SH) - Stack Hourglass [33].\\n\\n\\n\\nConfig\\n\\nMHFormer [20]\\n\\nMixSTE [51]\\n\\nMotionBERT [55]\\n\\nL\\n\\n3\\n\\n8\\n\\n5\\n\\nC\\n\\n512\\n\\n512\\n\\n256\\n\\nTraining Epoch\\n\\n15\\n\\n160\\n\\n120\\n\\nBatch Size\\n\\n210\\n\\n4\\n\\n4\\n\\nLR\\n\\n1×10−3\\n\\n4×10−5\\n\\n5×10−4\\n\\nOptimizer\\n\\nAmsgrad\\n\\nAdam\\n\\nAdam\\n\\nAugmentation\\n\\nFlip\\n\\nFlip\\n\\nFlip\\n\\n2D Detector\\n\\nCPN\\n\\nCPN\\n\\nSH\\n\\nSince our TRA is designed for seq2seq pipeline, it is unnecessary to add it to the model which is designed for seq2frame pipeline (e.g., MHFormer). To provide a com- prehensive analysis of our method, we report results with TPC and with both TPC and TRA. We denote the resulting models as follows:\\n\\n\\tHoT w. MixSTE (MixSTE + TPC + TRA),\\n\\n\\tHoT w. MotionBERT (MotionBERT + TPC + TRA), which are designed for seq2seq pipeline, and:\\n\\n\\tTPC w. MHFormer (MHFormer + TPC),\\n\\n\\tTPC w. MixSTE (MixSTE + TPC),\\n\\n\\tTPC w. MotionBERT (MotionBERT + TPC), which are designed for seq2frame pipeline.\\n\\n\\tAdditional Quantitative Results\\n\\nTraining Memory Cost and Training Time. To demon- strate the superiority of deploying our boosted VPTs on resource-limited devices, we report the training GPU mem- ory cost and training time per epoch in Table 9 (directly using their training settings). For instance, by equipping with HoT, MotionBERT achieves a memory cost reduction of 43.0% and a training time reduction of 47.5% while main-\\n\\ntaining the same performance. Note that we report the re- sults of our method using the default settings, i.e., {F =351, n=1, f =117} for MHFormer, {F =243, n=3, f =81} for\\n\\n\\n\\nTable 9. Comparison of GPU memory cost (G) and training time (min/epoch) on a single GeForce RTX 3090 GPU.\\n\\n\\n\\nMethod\\n\\nGPU Memory\\n\\nTraining Time\\n\\nMPJPE ↓\\n\\nMHFormer [20]\\n\\n24.1\\n\\n223.2\\n\\n43.0\\n\\nTPC w. MHFormer\\n\\n13.8 (↓ 42.7%)\\n\\n131.0 (↓ 39.7%)\\n\\n43.0\\n\\nMixSTE [51]\\n\\n11.4\\n\\n17.0\\n\\n40.9\\n\\nHoT w. MixSTE\\n\\n7.6 (↓ 33.3%)\\n\\n10.5 (↓ 38.2%)\\n\\n41.0\\n\\nTPC w. MixSTE\\n\\n7.3 (↓ 36.0%)\\n\\n10.1 (↓ 40.6%)\\n\\n40.4\\n\\nMotionBERT [55]\\n\\n10.7\\n\\n17.4\\n\\n39.8\\n\\nHoT w. MotionBERT\\n\\n6.1 (↓ 43.0%)\\n\\n8.9 (↓ 47.5%)\\n\\n39.8\\n\\nTPC w. MotionBERT\\n\\n5.7 (↓ 46.7%)\\n\\n8.4 (↓ 51.7%)\\n\\n39.2\\n\\nTable 10. Comparison of computational complexity and estimation accuracy.\\n\\n\\n\\n\\n\\nMethod\\n\\n\\t\\tF\\tf\\tParam (M) FLOPs (G)\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n81\\n\\n81\\n\\n33.70\\n\\n92.42\\n\\n42.7\\n\\nMixSTE [51]\\n\\n147\\n\\n147\\n\\n33.73\\n\\n167.72\\n\\n41.8\\n\\nMixSTE [51]\\n\\n243\\n\\n243\\n\\n33.78\\n\\n277.25\\n\\n40.9\\n\\n\\n\\nHoT w. MixSTE\\n\\n243\\n\\n81\\n\\n35.00\\n\\n167.52\\n\\n41.0\\n\\nTPC w. MixSTE\\n\\n243\\n\\n81\\n\\n33.78\\n\\n161.73\\n\\n40.4\\n\\n\\n\\n\\t{\\t}\\n\\n\\t{\\t}MixSTE, and F =243, n=1, f =81 for MotionBERT. We can also flexibly adjust the values of n and f to achieve a speed-accuracy trade-off that meets the specific demands of real-world applications (see Tables 12 and 13).\\n\\nComputation Complexity and Accuracy. In our main pa- per, we mainly report the results to show that our method can reduce FLOPs while achieving highly competitive or even better results (Tables 1, 2, 3, and 6). Here, we compare our method with MixSTE using the same number of representa- tive tokens and approximately the same number of FLOPs. To achieve this, we set the input frame number of the original MixSTE to F =81 and F =147, respectively. The results in Table 10 show that our method obtains better results under both settings, further demonstrating the importance of large receptive fields and the effectiveness of our method.\\n\\n\\tAdditional Ablation Study\\n\\nNumber of Recovered Tokens. In Table 11, we conduct the ablation study on the number of recovered tokens (f ′). Since f ′ differs from the input frames, we evaluate the performance under the seq2frame pipeline, which selects the 3D pose of the center frame as the final estimation. The results show that reducing f ′ slightly decreases the number of parameters, but the performance remains almost unchanged. Therefore, we choose f ′=243, which is more efficient and can be evaluated under the seq2seq pipeline.\\n\\nHyperparameters (n and f ). In Tables 2 and 3, we conduct ablation studies on the block index of representative tokens\\n\\n(n) under the seq2frame pipeline and on the number of repre- sentative tokens (f ) under the seq2seq pipeline, respectively. To systematically explore the hyperparameters, we further conduct the ablation studies on n under the seq2seq pipeline (Table 12) and on f under the seq2frame pipeline (Table 13).\\n\\n\\nTable 11. Ablation study on the number of recovered tokens (f ′) under seq2frame pipeline.\\n\\n\\n\\n\\n\\nMethod\\n\\n\\tParam (M)\\tFLOPs (G)\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n\\t33.78\\t277.25\\n\\n40.9\\n\\n\\n\\nHoT w. MixSTE (f ′=9)\\n\\n34.88\\n\\n163.33\\n\\n40.9\\n\\nHoT w. MixSTE (f ′=27)\\n\\n34.89\\n\\n163.66\\n\\n40.7\\n\\nHoT w. MixSTE (f ′=81)\\n\\n34.92\\n\\n164.62\\n\\n40.9\\n\\nHoT w. MixSTE (f ′=243)\\n\\n35.00\\n\\n167.52\\n\\n40.9\\n\\nAdditionally, we provide the results of training GPU memory cost and training time per epoch to demonstrate that the trade- off between computational complexity and performance can be adjusted flexibly on demand.\\n\\n\\tAdditional Visualization Results\\n\\nSelected Tokens. In Figure 7, we provide statistics visual- ization of selected tokens by taking some samples of con- secutive video frames as input with a temporal interval of 1 between neighboring samples. For more comprehensive observation, we further statistically visualize selected tokens of different token pruning strategies using random samples (temporal interval is set to 243), i.e., the neighboring sam- ples have no overlapping frames. The frame indexes and frequency count of frame indexes of selected tokens are shown at the top and bottom of Figure 9. The visualization figure of motion pruning (Figure 9 (c)) shows the most sig- nificant changes compared to Figure 7. The reason for this is that the random samples do not contain consecutive motion information. Interestingly, the visualization figures of TPC and motion pruning are also similar, but our TPC selects more tokens in the center frame and achieves much better performance.\\n\\nCluster Groups. The visualization in Figure 10 depicts cluster groups corresponding to varying numbers of repre- sentative tokens (f ). We observe that the cluster primarily groups neighboring tokens into the same group, as the nearby poses are similar. Moreover, it also groups some tokens that are relatively distant from each other into the same group based on their feature similarity.\\n\\n3D Pose Reconstruction. Figure 11 presents the qualita- tive comparison among the proposed HoT w. MixSTE and TPC w. MixSTE, and MixSTE [51] on Human3.6M dataset. Furthermore, Figure 12 shows the qualitative results on chal- lenging in-the-wild videos. These results confirm the ability of our method to produce accurate 3D pose estimations. However, there are some failure cases in challenging scenar- ios where our method cannot accurately estimate 3D human poses due to factors such as half body, rare poses, and large 2D detector error (Figure 13). We also provide visualiza- tions of recovering 3D human poses in Figure 14, which illustrate that our method can predict realistic 3D human poses of the entire sequence, thereby further demonstrating the effectiveness of the proposed TRA.\\n\\n\\n\\nTable 12. Ablation study on the block index of representative tokens (n) under seq2seq pipeline.\\n\\n\\n\\n\\n\\nMethod\\n\\nParam (M)\\n\\nFLOPs (G)\\n\\nFPS\\n\\nGPU Memory (G)\\n\\nTraining Time\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n33.78\\n\\n277.25\\n\\n10432\\n\\n11.4\\n\\n17.0\\n\\n40.9\\n\\n\\n\\nHoT w. MixSTE, n=1\\n\\n35.00\\n\\n121.31 (↓ 56.3%)\\n\\n20374 (↑ 95.3%)\\n\\n6.0 (↓ 47.4%)\\n\\n7.8 (↓ 54.1%)\\n\\n41.8\\n\\nHoT w. MixSTE, n=2\\n\\n35.00\\n\\n144.42 (↓ 47.9%)\\n\\n17724 (↑ 69.9%)\\n\\n6.8 (↓ 40.4%)\\n\\n9.2 (↓ 45.9%)\\n\\n41.6\\n\\nHoT w. MixSTE, n=3\\n\\n35.00\\n\\n167.52 (↓ 39.6%)\\n\\n15770 (↑ 51.2%)\\n\\n7.6 (↓ 33.3%)\\n\\n10.5 (↓ 38.2%)\\n\\n41.0\\n\\nHoT w. MixSTE, n=4\\n\\n35.00\\n\\n190.62 (↓ 31.2%)\\n\\n14094 (↑ 35.1%)\\n\\n8.5 (↓ 25.4%)\\n\\n12.0 (↓ 29.4%)\\n\\n41.4\\n\\nHoT w. MixSTE, n=5\\n\\n35.00\\n\\n213.72 (↓ 22.9%)\\n\\n12801 (↑ 22.7%)\\n\\n9.3 (↓ 18.4%)\\n\\n13.2 (↓ 22.4%)\\n\\n41.7\\n\\nHoT w. MixSTE, n=6\\n\\n35.00\\n\\n236.82 (↓ 14.6%)\\n\\n11673 (↑ 11.9%)\\n\\n10.0 (↓ 12.3%)\\n\\n14.7 (↓ 13.5%)\\n\\n41.6\\n\\nHoT w. MixSTE, n=7\\n\\n35.00\\n\\n259.93 (↓ 06.3%)\\n\\n10791 (↑ 03.4%)\\n\\n10.9 (↓ 04.4%)\\n\\n16.0 (↓ 05.9%)\\n\\n41.5\\n\\n\\n\\nTable 13. Ablation study on the number of representative tokens (f ) under seq2frame pipeline. Here, ∗ denotes the result without re-training.\\n\\n\\n\\n\\n\\nMethod\\n\\nParam (M)\\n\\nFLOPs (G)\\n\\nFPS\\n\\nGPU Memory (G)\\n\\nTraining Time\\n\\nMPJPE∗\\n\\nMPJPE ↓\\n\\n\\n\\nMixSTE [51]\\n\\n33.78\\n\\n277.25\\n\\n43\\n\\n11.4\\n\\n17.0\\n\\n40.7\\n\\n40.7\\n\\n\\n\\nTPC w. MixSTE, f =9\\n\\n33.78\\n\\n110.39 (↓ 60.2%)\\n\\n89 (↑ 107.0%)\\n\\n5.8 (↓ 49.1%)\\n\\n7.2 (↓ 57.6%)\\n\\n44.1\\n\\n41.5\\n\\nTPC w. MixSTE, f =16\\n\\n33.78\\n\\n115.38 (↓ 58.4%)\\n\\n88 (↑ 104.7%)\\n\\n5.8 (↓ 49.1%)\\n\\n7.5 (↓ 55.9%)\\n\\n42.7\\n\\n41.0\\n\\nTPC w. MixSTE, f =27\\n\\n33.78\\n\\n123.23 (↓ 55.6%)\\n\\n84 (↑ 95.3%)\\n\\n6.1 (↓ 46.5%)\\n\\n7.9 (↓ 53.5%)\\n\\n41.8\\n\\n40.5\\n\\nTPC w. MixSTE, f =61\\n\\n33.78\\n\\n147.47 (↓ 46.8%)\\n\\n75 (↑ 74.4%)\\n\\n7.0 (↓ 38.6%)\\n\\n9.3 (↓ 45.3%)\\n\\n41.2\\n\\n40.5\\n\\nTPC w. MixSTE, f =81\\n\\n33.78\\n\\n161.73 (↓ 41.7%)\\n\\n68 (↑ 58.1%)\\n\\n7.3 (↓ 36.0%)\\n\\n10.1 (↓ 40.6%)\\n\\n41.1\\n\\n40.4\\n\\nTPC w. MixSTE, f =121\\n\\n33.78\\n\\n190.26 (↓ 31.4%)\\n\\n62 (↑ 44.2%)\\n\\n8.3 (↓ 27.2%)\\n\\n11.7 (↓ 31.2%)\\n\\n40.9\\n\\n40.4\\n\\nTPC w. MixSTE, f =135\\n\\n33.78\\n\\n200.24 (↓ 27.8%)\\n\\n58 (↑ 34.9%)\\n\\n8.5 (↓ 25.4%)\\n\\n12.4 (↓ 27.1%)\\n\\n40.9\\n\\n40.2\\n\\n\\n\\n\\t\\t\\t(a) Uniform Sampling\\t(b) Attention Score Pruning\\t(c) Motion Pruning\\t(d) The Proposed TPC\\n\\nFigure 9. Statistics visualization of selected tokens for different token pruning strategies. Top: Frame indexes of selected tokens for some samples (140 samples) of video sequences (243 frames). Blue points represent selected tokens and white points represent pruned tokens. Bottom: Frequency count of frame indexes of selected tokens for these samples.\\n\\n\\n\\n\\n\\n\\n\\n\\tf = 3\\n\\n\\t\\nf = 9\\n\\n\\t\\nf = 81\\n\\n\\n\\nFigure 10. Visualization of cluster groups for the different numbers of representative tokens f . In each row, points of the same color represent the same cluster group.\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tInput\\tMixSTE\\tHoT w. MixSTE\\tTPC w. MixSTE\\tGround Truth\\n\\nFigure 11. Qualitative comparison among the previous state-of-the-art method (MixSTE [51]), our HoT w. MixSTE, and our TPC w. MixSTE on Human3.6M dataset.\\n\\n\\n\\n\\n\\nm\\n\\nm\\n\\nFigure 12. Qualitative results of our method on challenging in-the-wild videos.\\n\\n\\n\\n\\n\\n\\t  \\t  \\n\\n\\n\\nInput\\n\\n\\t\\nMixSTE\\tHoT w. MixSTE\\n\\n\\nInput\\n\\n\\t\\nMixSTE\\tHoT w. MixSTE\\n\\n\\n\\nFigure 13. Failure cases in challenging scenarios.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nh\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFigure 14. Visualization of input images, estimated 3D poses (cyan), and ground truth 3D poses (black) from three video sequences. The 2D poses of selected frames are colored in red, and the 2D poses of pruned frames are colored in gray. The 3D poses of selected frames are highlighted with red rectangular boxes.\\n\\n\\n\\n10\\n\\n10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_path, \"w\") as file:\n",
        "  file.write(doc)"
      ],
      "metadata": {
        "id": "1ZpzK5hkGZ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latex to text conversion"
      ],
      "metadata": {
        "id": "1_r_wxyMGVzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM93vAA8akBR",
        "outputId": "bf9c43dc-40da-43aa-9bab-a00163326d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.12-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pypandoc"
      ],
      "metadata": {
        "id": "zGGUbka6bOXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latex_path = \"/content/drive/MyDrive/SPIDER/docs/acl.tex\"\n",
        "text_path = \"/content/acl.txt\""
      ],
      "metadata": {
        "id": "etYr96hsJrLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_latex_to_text(latex_code):\n",
        "    try:\n",
        "        # Convert LaTeX to plain text using pandoc\n",
        "        text_content = pypandoc.convert_text(latex_code, 'plain', format='latex')\n",
        "        return text_content\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting LaTeX to text: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define your LaTeX code\n",
        "with open(latex_path) as file:\n",
        "  latex_code = file.read()\n",
        "\n",
        "# Convert LaTeX to text\n",
        "text_content = convert_latex_to_text(latex_code)\n",
        "\n",
        "# Print the text content\n",
        "with open(text_path, \"w\") as file:\n",
        "  file.write(text_content) # there are some warnings"
      ],
      "metadata": {
        "id": "9civuP93XozC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690cf929-604d-4e86-8ddb-b709ab3db040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] Could not convert TeX math '\\vec{she}', rendering as TeX\n",
            "WARNING:pypandoc:Could not convert TeX math '\\vec{she}', rendering as TeX\n",
            "[WARNING] Could not convert TeX math '\\vec{he}', rendering as TeX\n",
            "WARNING:pypandoc:Could not convert TeX math '\\vec{he}', rendering as TeX\n",
            "[WARNING] Could not convert TeX math '\\cos(\\vec{w},\\vec{she}) - \\cos(\\vec{w},\\vec{he})', rendering as TeX\n",
            "WARNING:pypandoc:Could not convert TeX math '\\cos(\\vec{w},\\vec{she}) - \\cos(\\vec{w},\\vec{he})', rendering as TeX\n",
            "[WARNING] Could not convert TeX math '\\frac{1}{N_W} \\sum_{w \\in W}(\\cos(\\vec{w},\\vec{she}) - \\cos(\\vec{w},\\vec{he}))^2,', rendering as TeX\n",
            "\n",
            "WARNING:pypandoc:Could not convert TeX math '\\frac{1}{N_W} \\sum_{w \\in W}(\\cos(\\vec{w},\\vec{she}) - \\cos(\\vec{w},\\vec{he}))^2,', rendering as TeX\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_content"
      ],
      "metadata": {
        "id": "1m6XwnxWqAg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "edc64efd-c080-4e81-957c-7cffdb01eb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introduction\\n\\nTransformer models have received increased attention over the recent\\nyears. Much progress was achieved by improvements to model\\narchitectures, components, and algorithms such as from RNN to LSTM or\\nGRU\\xa0, and from seq2seq\\xa0 to attention\\xa0, and GLM\\xa02.0\\xa0 to name a few.\\nProgress also resulted from vastly increasing parameters, such as GPT-2\\xa0\\nwith 1.5 billion, GPT-3\\xa0 with 175 billion, and Google Switch\\xa0 with 1.6\\ntrillion parameters among others.\\n\\nHowever, the training of a transformer model from scratch requires\\namounts of training data and computing power by far exceeding the scope\\nof individual application development. Furthermore, while pre-trained\\nmodels perform well when applying basic NLP tasks to common and broadly\\ndefined domains, they tend not to meet the requirements of more complex\\ntasks applied to less common and more narrowly defined domains.\\n\\nA key element supporting a wide variety of applications is the\\nsimplicity with which a pre-trained model may be turned into a\\nspecial-purpose model by means of fine-tuning: the act of progressively\\nadapting a subset of model weights based on a task- or domain-specific\\ndataset.\\n\\nFor example, a domain-specific question answering (q&a) model may be\\nobtained using a fine-tuning dataset containing pairs of questions and\\nanswers related to that domain. However, while such a model may achieve\\nan acceptable answering performance, it is unclear which discriminating\\nfeatures it is able to capture from questions, and what knowledge is\\napplied when answers are generated.\\n\\nThis makes it difficult to create a q&a model when particular\\nrequirements are imposed to answer content and wording. Furthermore, if\\nquestion contexts such as topics of discourse, sentiments or user\\neducation should affect the answer content and wording, the fine-tuning\\ndataset must include all combinations of context, content and wording.\\nHowever, increasing the complexity of the dataset complicates the\\npredictability of model responses.\\n\\nIn this paper we introduce a method for building transformer\\napplications where model responses are controllable while complex tasks\\nare performed. We present the results of initial experiments conducted\\nwith GPT-3 to reduce gender bias in English texts. The results suggest\\nfurther experiments with other complex tasks, which will be reported in\\na more comprehensive publication.\\n\\nThe rest of the paper is structured as follows. In Section\\xa02 we present\\nsome basic notions and challenges dealing with gender bias removal,\\nwhich represents for us an exemplary task where to test our novel\\napproach, introduced in Section\\xa03. In Section\\xa04 we compare our approach\\nto two baseline architectures and we show the performance in terms of\\nbias reduction. In Section\\xa05 we discuss our preliminary results and\\nenvision further extension of the approach.\\n\\nGender Bias Removal as NLP Task\\n\\nWe introduce gender bias removal as an NLP task where multiple input\\nfeatures must be taken into account, while model responses must be\\ncontrolled.\\n\\nPrevious work on gender bias focused on specific NLP aspects such as\\nword embeddings\\xa0, coreference resolutions\\xa0, and part-of-speech and\\ndependency parsing\\xa0. However, these approaches share several limitations\\nregarding their effectiveness of removing bias in texts\\xa0. First, they\\ntend to conflate different conversational dimensions of gender bias and\\nare thus unable to detect subtle pragmatic differences. Second, they are\\noften limited to explicitly binarly gendered words while many words are\\nnot explicitly gendered. Third, focusing on the male-female gender\\ndirection they neglect the impact of words that have a gender\\norientation but are not necessarily unfairly biased.\\n\\nA more general framework was proposed in\\xa0 where textual gender bias is\\ndecomposed along three different pragmatic and semantic dimensions, such\\nas bias 1.\\xa0from the gender of the person being spoken about, 2.\\xa0from the\\ngender of the person being spoken to, and 3.\\xa0from the gender of the\\nspeaker. It was shown that the distinction of gender bias along multiple\\ndimensions generates better and more fine-grained gender bias\\nclassifiers. Consequently, we define gender bias removal as a complex\\nNLP, which can benefit from a multi-step approach, including bias\\ndetection, classification and reformulation.\\n\\nIn this paper we characterise the occurrence of gender bias in two\\naspects. One aspect is the identification of the bias type, such as the\\nuse of gender-exclusive keywords for a gender-neutral entity (explicit\\nbias, ), or the reference to a gender-neutral term through a gendered\\npronoun (generalisation bias, ), or expressions of well meant attitudes\\ntowards one gender guided by stereotypes (benevolent sexism, ). The\\nsecond aspect then captures the actual terms having such a type of bias.\\nAs a result of bias being characterised in two aspects, the treatment of\\ntext must be specific to their combination. For each pair of bias type\\nand terms having this type of bias, an appropriate reformulation must be\\napplied.\\n\\nApproach\\n\\nThe key to our approach is to break down a complex task into simpler\\nsubtasks. For each subtask, a dataset is created, on which a\\ntask-specific transformer model is fine-tuned. Since a subtask is less\\ncomplex, these task-specific datasets remain small, and the behaviour of\\neach model is thus more controllable. Once all models are fine-tuned,\\nthey can be lined up to complete the overall task.\\n\\nDebiasing\\n\\nWe define the three subtasks bias classification, bias extraction, and\\ntext reformulation for which dedicated transformer models are used.\\n\\nFor the first subtask, a model is used to identify if and which type of\\nbias a sentence has: gender generalization bias, explicit gender bias,\\nbenevolent sexism, or no bias. If no bias is detected, the debiasing\\nprocess is halted. The second subtask is to extract the terms that\\nconcern a bias. For each bias type there is a model able to perform a\\ntype-specific extraction. Thus, the bias type identified in the first\\nsubtask is used to select the appropriate model. As third subtask, the\\nbias is removed using a model able to reformulate text. Similar to the\\nsecond subtask, a model is available for each bias type. The type\\nidentified in the first subtask is thus used to select the appropriate\\nmodel. The bias-carrying terms extracted in the second subtask are\\nprovided as input in addition to the text to be reformulated. Since a\\ntext may contain several biases of different types, an iterative\\napproach is used, in which the text is repeatedly classified and treated\\nuntil it results bias-free. Figure\\xa0[fig:model3] shows the transformer\\narchitecture and debiasing process.\\n\\n[image]\\n\\nAll models are GPT-3 davinci models, accessed through the OpenAI API.\\nFor details regarding GPT-3 architecture we refer to the original paper\\n. The temperature T was set to 0.2, TopP to 1, and BestOf to 1. The\\nfine-tuning datasets for each subtask contained 10 input-output\\nexamples. The dataset for the classification model that performed the\\nfirst subtask contained 10 examples of each bias type, yielding a total\\nof 40 examples given in the prompt as fine-tuning dataset.\\n\\nData\\n\\nTo obtain the fine-tuning and test datasets, we first created a\\ngender-bias dataset using Wikipedia’s neutral point of view (NPOV)\\nedits. An NPOV edit is a sentence that has been reported as biased and\\nhas therefore been changed by a Wikipedia contributor. Both versions of\\nthe sentence are stored in the revision history of the NPOV edits. To\\nfocus on gender bias, sentences that did not contain gender pronouns\\nwere excluded, which determined a final size of 45’539 sentence-pairs\\n(Wikipedia NPOV dataset). By comparing the biased and unbiased forms of\\nthe sentence, the bias-inducing terms and their unbiased substitutes\\nwere extracted.\\n\\nFinally, a bias-labeled subset from the Wikipedia NPOV dataset was\\nselected, in which all bias types were represented with equal frequency.\\nIn this subset, the biased sentences were manually labeled according to\\nthe three types of bias. Overall, the bias-labeled subset contained 40\\nexamples to be used in the prompts for fine-tuning and 92 examples as\\ntest dataset.\\n\\nThe fine-tuning dataset for the first subtask (bias classification) was\\ncomposed of biased sentences paired with the bias types. The three\\ndatasets for the second subtask (bias extraction) contained the biased\\nsentences paired with the bias-inducing terms, grouped by the bias type.\\nFor the third subtask (text reformulation), three datasets were created\\nwhere the biased sentences and the bias-inducing terms were paired with\\nthe unbiased sentences, grouped by bias type.\\n\\nFor the evaluation, 92 bias-labeled NPOV edits were used to create a\\ntest dataset containing the biased and unbiased sentences, the\\nbias-inducing terms, and labeled with the bias types.\\n\\nBias Measurement\\n\\nTwo different approaches were developed to measure the bias before and\\nafter treatment.\\n\\nThe first approach validates each subtask using the F1 score. In the\\nclassification and extraction subtasks, this is done by comparing the\\nmodel output to the expected output given in the test dataset. The\\nreformulation subtask is validated by comparing reformulated sentences\\nto the unbiased ones from the test dataset. Because of the diversity of\\nhow sentences can be rephrased, the bias classifier was again applied to\\nthe reformulated sentence. As a result, if a reformulated sentence did\\nnot match the unbiased sentence from the test dataset, we could still\\nprovide an indication of whether the bias had been removed.\\n\\nThe second approach uses GloVe word embeddings\\xa0 to quantify the result\\nof debiasing. Following , we identified a gender direction (subspace)\\nbased on the gendered word pair she-he. This subspace is then used to\\nevaluate the position of words that typically have a strong gender\\nassociation in terms of gender direction.\\n\\nFor each word w, we computed the cosine similarity between its vector\\nrepresentation w⃗ and the vector representation of the gender pronouns\\n$\\\\vec{she}$ and $\\\\vec{he}$. The degree of a word’s gender-neutrality,\\ncalled word neutrality, is defined as\\n$\\\\cos(\\\\vec{w},\\\\vec{she}) - \\\\cos(\\\\vec{w},\\\\vec{he})$ where neutral words\\ntend towards zero. Aggregating and normalizing over the whole vocabulary\\nW\\n$$\\\\frac{1}{N_W} \\\\sum_{w \\\\in W}(\\\\cos(\\\\vec{w},\\\\vec{she}) - \\\\cos(\\\\vec{w},\\\\vec{he}))^2,$$\\nwith N_(W) being the vocabulary size, we get an overall measure for\\ngender bias in a text, namely the mean squared word neutrality (MSWN).\\n\\nResults and Evaluation\\n\\nTo evaluate our approach, we designed two additional transformer\\narchitectures as base lines. The first one (M-1) consists of a single\\ntransformer model fine-tuned to perform debiasing as one single task.\\nThe second one (M-2) is a double-transformer system, composed by a first\\nmodel identifying the bias type which selects the second model for\\ntype-specific reformulation. In what follows, we refer to our approach\\nas M-3. We debiased the 92 sentences from the test dataset using M-1,\\nM-2, and M-3. We then used the bias measurements F1 Score and Mean\\nSquared Word Neutrality introduced above to quantify each architecture’s\\nperformance and the debiasing process.\\n\\nF1 Scores\\n\\nDebiasing is deemed successful when the bias-inducing terms are removed\\nand replaced with unbiased alternatives, resulting in an unbiased\\nsentence. The F1 scores are shown in Table\\xa01. In comparison, the\\nbenefits of each split in subtasks emerges clearly: 100% improvement in\\nmicro averaged F1 score from M-1 to M-2, and an additional 50%\\nimprovement from M-2 to M-3.\\n\\n  Gender Bias Type         M-1    M-2    M-3\\n  ----------------------- ------ ------ ------\\n  Benevolent sexism        0.09   0.57   0.87\\n  Explicit gender bias     0.18   0.27   0.95\\n  Gender generalization    0.65   0.87   0.91\\n  Micro average            0.31   0.57   0.91\\n\\n  : Comparison of debiasing F1 scores for the approch proposed in this\\n  paper (M-3) and the two baselines (M-1 and M-2).\\n\\nInterestingly, differences in F1-score occur also along different gender\\nbias type. Apparently, some more subtle forms of gender bias require a\\nmore extensive treatment than others. This is the case for example of\\nbenevolent sexism or explicit gender bias, which in most of the cases\\nfail to be corrected by using the base lines models. However, for the\\nsimpler case of gender generalization bias even a single transformer\\nprovide satisfactory results.\\n\\nThe introduction of the bias classification transformer in M-2 and M-3,\\nimproves the performance at reformulation stage, also due to the fact\\nthat treating each bias type separately, allows the prompt to contain\\nmore examples, providing therefore more fine-tuning training.\\nConsequently, a multi-transformers approach presents the advantage to be\\nmore flexible and scalable across different complexity level within a\\ntask.\\n\\nMean Squared Word Neutrality\\n\\nWe computed the MSWN for the test dataset as well as for the complete\\nWikipedia NPOV dataset, see values in Table\\xa02. We are thus able to\\ncompare the transformer-based approach to the work of the Wikipedia\\ncontributors. For comparability, we considered as vocabulary base the\\nprofession titles from\\xa0, and descriptive words from\\xa0. Of the original\\nlists considered, 49 professions and 67 descriptions appear in the\\nWikipedia NPOV dataset, while 8 and 7 resp. are found in the test\\ndataset.\\n\\nAn MSWN closer to zero means less gender-bias encountered. Independent\\nof the set of words considered, the MSWNs get closer to zero after\\ntreatment. Moreover, this does not only apply to debiasing performed by\\nhumans (Wikipedia NPOV dataset) but also to the sentences from the test\\ndataset which were debiased following our M-3 approach.\\n\\n  Dataset              Professions   Descriptions  \\n  ------------------- ------------- -------------- --\\n  NPOV                   0.0090         0.0057     \\n  NPOV Debiased          0.0049         0.0040     \\n  Test Set               0.0207         0.0085     \\n  Test Set Debiased      0.0156         0.0065     \\n\\n  :  MSWN in Wikipedia NPOV dataset (biased and debiased by humans), and\\n  in the test dataset (biased and debiased using M-3).\\n\\nDiscussion and Conclusions\\n\\nIn this paper we presented a new approach to address complex NLP tasks\\nsuch as gender-bias removal. The multitude of aspects which characterize\\ndifferent type of gender bias proved to favour our iterative multi-step\\napproach, where the ultimate task (bias removal) is split into simpler\\nsubtasks. Each subtask is performed by a dedicated, specifically\\nfine-tuned transformer model. Our approach proved to be effective when\\nthe models were fine-tuned using a handful of sentences, in contrast to\\nusing a single model, which potentially need more fine-tuning data to\\nprovide comparable results.\\n\\nThe advantage presented from this task-splitting is not only to have\\neach transformer do a simpler task, but also the possibility to generate\\nmore straightforward input-output data combinations for fine-tuning.\\nMoreover, our approach can be extended by including an arbitrary number\\nof bias types and treatments.\\n\\nHowever, our approach currently treats multiple biases one by one,\\niteratively. This could be improved if the approach would be extended to\\ntreat multiple biases at once.\\n\\nThe results of applying our approach to bias removal in natural language\\ntexts indicated that the method proposed is effective. In order to\\nexplore its applicability in more general settings, we aim to apply it\\nto other NLP tasks and use different transformer models.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Frequency Estimation\n",
        "\n",
        "- Here we need to find different methods to figure out word frequencies in a text/doc.\n",
        "- I am using the text from previously generated text."
      ],
      "metadata": {
        "id": "MXcTRdvAKp06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **Counter** class is part of the **collections** module in Python, and it provides a convenient way to count the occurrences of elements in a collection (e.g., a list or a string).\n",
        "- The **string** module provides additional functions for string manipulation."
      ],
      "metadata": {
        "id": "XzhqsGYlsiP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import string"
      ],
      "metadata": {
        "id": "rz0LW5TsNMzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "RaSKP9CbK7ND",
        "outputId": "693fd1d3-1753-480e-ccf5-41aee4c41911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introduction\\n\\nTransformer models have received increased attention over the recent\\nyears. Much progress was achieved by improvements to model\\narchitectures, components, and algorithms such as from RNN to LSTM or\\nGRU\\xa0, and from seq2seq\\xa0 to attention\\xa0, and GLM\\xa02.0\\xa0 to name a few.\\nProgress also resulted from vastly increasing parameters, such as GPT-2\\xa0\\nwith 1.5 billion, GPT-3\\xa0 with 175 billion, and Google Switch\\xa0 with 1.6\\ntrillion parameters among others.\\n\\nHowever, the training of a transformer model from scratch requires\\namounts of training data and computing power by far exceeding the scope\\nof individual application development. Furthermore, while pre-trained\\nmodels perform well when applying basic NLP tasks to common and broadly\\ndefined domains, they tend not to meet the requirements of more complex\\ntasks applied to less common and more narrowly defined domains.\\n\\nA key element supporting a wide variety of applications is the\\nsimplicity with which a pre-trained model may be turned into a\\nspecial-purpose model by means of fine-tuning: the act of progressively\\nadapting a subset of model weights based on a task- or domain-specific\\ndataset.\\n\\nFor example, a domain-specific question answering (q&a) model may be\\nobtained using a fine-tuning dataset containing pairs of questions and\\nanswers related to that domain. However, while such a model may achieve\\nan acceptable answering performance, it is unclear which discriminating\\nfeatures it is able to capture from questions, and what knowledge is\\napplied when answers are generated.\\n\\nThis makes it difficult to create a q&a model when particular\\nrequirements are imposed to answer content and wording. Furthermore, if\\nquestion contexts such as topics of discourse, sentiments or user\\neducation should affect the answer content and wording, the fine-tuning\\ndataset must include all combinations of context, content and wording.\\nHowever, increasing the complexity of the dataset complicates the\\npredictability of model responses.\\n\\nIn this paper we introduce a method for building transformer\\napplications where model responses are controllable while complex tasks\\nare performed. We present the results of initial experiments conducted\\nwith GPT-3 to reduce gender bias in English texts. The results suggest\\nfurther experiments with other complex tasks, which will be reported in\\na more comprehensive publication.\\n\\nThe rest of the paper is structured as follows. In Section\\xa02 we present\\nsome basic notions and challenges dealing with gender bias removal,\\nwhich represents for us an exemplary task where to test our novel\\napproach, introduced in Section\\xa03. In Section\\xa04 we compare our approach\\nto two baseline architectures and we show the performance in terms of\\nbias reduction. In Section\\xa05 we discuss our preliminary results and\\nenvision further extension of the approach.\\n\\nGender Bias Removal as NLP Task\\n\\nWe introduce gender bias removal as an NLP task where multiple input\\nfeatures must be taken into account, while model responses must be\\ncontrolled.\\n\\nPrevious work on gender bias focused on specific NLP aspects such as\\nword embeddings\\xa0, coreference resolutions\\xa0, and part-of-speech and\\ndependency parsing\\xa0. However, these approaches share several limitations\\nregarding their effectiveness of removing bias in texts\\xa0. First, they\\ntend to conflate different conversational dimensions of gender bias and\\nare thus unable to detect subtle pragmatic differences. Second, they are\\noften limited to explicitly binarly gendered words while many words are\\nnot explicitly gendered. Third, focusing on the male-female gender\\ndirection they neglect the impact of words that have a gender\\norientation but are not necessarily unfairly biased.\\n\\nA more general framework was proposed in\\xa0 where textual gender bias is\\ndecomposed along three different pragmatic and semantic dimensions, such\\nas bias 1.\\xa0from the gender of the person being spoken about, 2.\\xa0from the\\ngender of the person being spoken to, and 3.\\xa0from the gender of the\\nspeaker. It was shown that the distinction of gender bias along multiple\\ndimensions generates better and more fine-grained gender bias\\nclassifiers. Consequently, we define gender bias removal as a complex\\nNLP, which can benefit from a multi-step approach, including bias\\ndetection, classification and reformulation.\\n\\nIn this paper we characterise the occurrence of gender bias in two\\naspects. One aspect is the identification of the bias type, such as the\\nuse of gender-exclusive keywords for a gender-neutral entity (explicit\\nbias, ), or the reference to a gender-neutral term through a gendered\\npronoun (generalisation bias, ), or expressions of well meant attitudes\\ntowards one gender guided by stereotypes (benevolent sexism, ). The\\nsecond aspect then captures the actual terms having such a type of bias.\\nAs a result of bias being characterised in two aspects, the treatment of\\ntext must be specific to their combination. For each pair of bias type\\nand terms having this type of bias, an appropriate reformulation must be\\napplied.\\n\\nApproach\\n\\nThe key to our approach is to break down a complex task into simpler\\nsubtasks. For each subtask, a dataset is created, on which a\\ntask-specific transformer model is fine-tuned. Since a subtask is less\\ncomplex, these task-specific datasets remain small, and the behaviour of\\neach model is thus more controllable. Once all models are fine-tuned,\\nthey can be lined up to complete the overall task.\\n\\nDebiasing\\n\\nWe define the three subtasks bias classification, bias extraction, and\\ntext reformulation for which dedicated transformer models are used.\\n\\nFor the first subtask, a model is used to identify if and which type of\\nbias a sentence has: gender generalization bias, explicit gender bias,\\nbenevolent sexism, or no bias. If no bias is detected, the debiasing\\nprocess is halted. The second subtask is to extract the terms that\\nconcern a bias. For each bias type there is a model able to perform a\\ntype-specific extraction. Thus, the bias type identified in the first\\nsubtask is used to select the appropriate model. As third subtask, the\\nbias is removed using a model able to reformulate text. Similar to the\\nsecond subtask, a model is available for each bias type. The type\\nidentified in the first subtask is thus used to select the appropriate\\nmodel. The bias-carrying terms extracted in the second subtask are\\nprovided as input in addition to the text to be reformulated. Since a\\ntext may contain several biases of different types, an iterative\\napproach is used, in which the text is repeatedly classified and treated\\nuntil it results bias-free. Figure\\xa0[fig:model3] shows the transformer\\narchitecture and debiasing process.\\n\\n[image]\\n\\nAll models are GPT-3 davinci models, accessed through the OpenAI API.\\nFor details regarding GPT-3 architecture we refer to the original paper\\n. The temperature T was set to 0.2, TopP to 1, and BestOf to 1. The\\nfine-tuning datasets for each subtask contained 10 input-output\\nexamples. The dataset for the classification model that performed the\\nfirst subtask contained 10 examples of each bias type, yielding a total\\nof 40 examples given in the prompt as fine-tuning dataset.\\n\\nData\\n\\nTo obtain the fine-tuning and test datasets, we first created a\\ngender-bias dataset using Wikipedia’s neutral point of view (NPOV)\\nedits. An NPOV edit is a sentence that has been reported as biased and\\nhas therefore been changed by a Wikipedia contributor. Both versions of\\nthe sentence are stored in the revision history of the NPOV edits. To\\nfocus on gender bias, sentences that did not contain gender pronouns\\nwere excluded, which determined a final size of 45’539 sentence-pairs\\n(Wikipedia NPOV dataset). By comparing the biased and unbiased forms of\\nthe sentence, the bias-inducing terms and their unbiased substitutes\\nwere extracted.\\n\\nFinally, a bias-labeled subset from the Wikipedia NPOV dataset was\\nselected, in which all bias types were represented with equal frequency.\\nIn this subset, the biased sentences were manually labeled according to\\nthe three types of bias. Overall, the bias-labeled subset contained 40\\nexamples to be used in the prompts for fine-tuning and 92 examples as\\ntest dataset.\\n\\nThe fine-tuning dataset for the first subtask (bias classification) was\\ncomposed of biased sentences paired with the bias types. The three\\ndatasets for the second subtask (bias extraction) contained the biased\\nsentences paired with the bias-inducing terms, grouped by the bias type.\\nFor the third subtask (text reformulation), three datasets were created\\nwhere the biased sentences and the bias-inducing terms were paired with\\nthe unbiased sentences, grouped by bias type.\\n\\nFor the evaluation, 92 bias-labeled NPOV edits were used to create a\\ntest dataset containing the biased and unbiased sentences, the\\nbias-inducing terms, and labeled with the bias types.\\n\\nBias Measurement\\n\\nTwo different approaches were developed to measure the bias before and\\nafter treatment.\\n\\nThe first approach validates each subtask using the F1 score. In the\\nclassification and extraction subtasks, this is done by comparing the\\nmodel output to the expected output given in the test dataset. The\\nreformulation subtask is validated by comparing reformulated sentences\\nto the unbiased ones from the test dataset. Because of the diversity of\\nhow sentences can be rephrased, the bias classifier was again applied to\\nthe reformulated sentence. As a result, if a reformulated sentence did\\nnot match the unbiased sentence from the test dataset, we could still\\nprovide an indication of whether the bias had been removed.\\n\\nThe second approach uses GloVe word embeddings\\xa0 to quantify the result\\nof debiasing. Following , we identified a gender direction (subspace)\\nbased on the gendered word pair she-he. This subspace is then used to\\nevaluate the position of words that typically have a strong gender\\nassociation in terms of gender direction.\\n\\nFor each word w, we computed the cosine similarity between its vector\\nrepresentation w⃗ and the vector representation of the gender pronouns\\n$\\\\vec{she}$ and $\\\\vec{he}$. The degree of a word’s gender-neutrality,\\ncalled word neutrality, is defined as\\n$\\\\cos(\\\\vec{w},\\\\vec{she}) - \\\\cos(\\\\vec{w},\\\\vec{he})$ where neutral words\\ntend towards zero. Aggregating and normalizing over the whole vocabulary\\nW\\n$$\\\\frac{1}{N_W} \\\\sum_{w \\\\in W}(\\\\cos(\\\\vec{w},\\\\vec{she}) - \\\\cos(\\\\vec{w},\\\\vec{he}))^2,$$\\nwith N_(W) being the vocabulary size, we get an overall measure for\\ngender bias in a text, namely the mean squared word neutrality (MSWN).\\n\\nResults and Evaluation\\n\\nTo evaluate our approach, we designed two additional transformer\\narchitectures as base lines. The first one (M-1) consists of a single\\ntransformer model fine-tuned to perform debiasing as one single task.\\nThe second one (M-2) is a double-transformer system, composed by a first\\nmodel identifying the bias type which selects the second model for\\ntype-specific reformulation. In what follows, we refer to our approach\\nas M-3. We debiased the 92 sentences from the test dataset using M-1,\\nM-2, and M-3. We then used the bias measurements F1 Score and Mean\\nSquared Word Neutrality introduced above to quantify each architecture’s\\nperformance and the debiasing process.\\n\\nF1 Scores\\n\\nDebiasing is deemed successful when the bias-inducing terms are removed\\nand replaced with unbiased alternatives, resulting in an unbiased\\nsentence. The F1 scores are shown in Table\\xa01. In comparison, the\\nbenefits of each split in subtasks emerges clearly: 100% improvement in\\nmicro averaged F1 score from M-1 to M-2, and an additional 50%\\nimprovement from M-2 to M-3.\\n\\n  Gender Bias Type         M-1    M-2    M-3\\n  ----------------------- ------ ------ ------\\n  Benevolent sexism        0.09   0.57   0.87\\n  Explicit gender bias     0.18   0.27   0.95\\n  Gender generalization    0.65   0.87   0.91\\n  Micro average            0.31   0.57   0.91\\n\\n  : Comparison of debiasing F1 scores for the approch proposed in this\\n  paper (M-3) and the two baselines (M-1 and M-2).\\n\\nInterestingly, differences in F1-score occur also along different gender\\nbias type. Apparently, some more subtle forms of gender bias require a\\nmore extensive treatment than others. This is the case for example of\\nbenevolent sexism or explicit gender bias, which in most of the cases\\nfail to be corrected by using the base lines models. However, for the\\nsimpler case of gender generalization bias even a single transformer\\nprovide satisfactory results.\\n\\nThe introduction of the bias classification transformer in M-2 and M-3,\\nimproves the performance at reformulation stage, also due to the fact\\nthat treating each bias type separately, allows the prompt to contain\\nmore examples, providing therefore more fine-tuning training.\\nConsequently, a multi-transformers approach presents the advantage to be\\nmore flexible and scalable across different complexity level within a\\ntask.\\n\\nMean Squared Word Neutrality\\n\\nWe computed the MSWN for the test dataset as well as for the complete\\nWikipedia NPOV dataset, see values in Table\\xa02. We are thus able to\\ncompare the transformer-based approach to the work of the Wikipedia\\ncontributors. For comparability, we considered as vocabulary base the\\nprofession titles from\\xa0, and descriptive words from\\xa0. Of the original\\nlists considered, 49 professions and 67 descriptions appear in the\\nWikipedia NPOV dataset, while 8 and 7 resp. are found in the test\\ndataset.\\n\\nAn MSWN closer to zero means less gender-bias encountered. Independent\\nof the set of words considered, the MSWNs get closer to zero after\\ntreatment. Moreover, this does not only apply to debiasing performed by\\nhumans (Wikipedia NPOV dataset) but also to the sentences from the test\\ndataset which were debiased following our M-3 approach.\\n\\n  Dataset              Professions   Descriptions  \\n  ------------------- ------------- -------------- --\\n  NPOV                   0.0090         0.0057     \\n  NPOV Debiased          0.0049         0.0040     \\n  Test Set               0.0207         0.0085     \\n  Test Set Debiased      0.0156         0.0065     \\n\\n  :  MSWN in Wikipedia NPOV dataset (biased and debiased by humans), and\\n  in the test dataset (biased and debiased using M-3).\\n\\nDiscussion and Conclusions\\n\\nIn this paper we presented a new approach to address complex NLP tasks\\nsuch as gender-bias removal. The multitude of aspects which characterize\\ndifferent type of gender bias proved to favour our iterative multi-step\\napproach, where the ultimate task (bias removal) is split into simpler\\nsubtasks. Each subtask is performed by a dedicated, specifically\\nfine-tuned transformer model. Our approach proved to be effective when\\nthe models were fine-tuned using a handful of sentences, in contrast to\\nusing a single model, which potentially need more fine-tuning data to\\nprovide comparable results.\\n\\nThe advantage presented from this task-splitting is not only to have\\neach transformer do a simpler task, but also the possibility to generate\\nmore straightforward input-output data combinations for fine-tuning.\\nMoreover, our approach can be extended by including an arbitrary number\\nof bias types and treatments.\\n\\nHowever, our approach currently treats multiple biases one by one,\\niteratively. This could be improved if the approach would be extended to\\ntreat multiple biases at once.\\n\\nThe results of applying our approach to bias removal in natural language\\ntexts indicated that the method proposed is effective. In order to\\nexplore its applicability in more general settings, we aim to apply it\\nto other NLP tasks and use different transformer models.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_word_freq(text):\n",
        "    # Remove punctuation and convert to lowercase (punctuations are : . , ! ? ( ) - ... )\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "\n",
        "    # Split the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_frequencies = Counter(words)\n",
        "\n",
        "    return word_frequencies"
      ],
      "metadata": {
        "id": "p843ZyM0NQ3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_word_freq(text_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94zze_GELMmI",
        "outputId": "f21065e2-ea03-4a30-8ac6-2381ce3a266a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'introduction': 2,\n",
              "         'transformer': 13,\n",
              "         'models': 9,\n",
              "         'have': 4,\n",
              "         'received': 1,\n",
              "         'increased': 1,\n",
              "         'attention': 2,\n",
              "         'over': 2,\n",
              "         'the': 164,\n",
              "         'recent': 1,\n",
              "         'years': 1,\n",
              "         'much': 1,\n",
              "         'progress': 2,\n",
              "         'was': 7,\n",
              "         'achieved': 1,\n",
              "         'by': 17,\n",
              "         'improvements': 1,\n",
              "         'to': 75,\n",
              "         'model': 26,\n",
              "         'architectures': 3,\n",
              "         'components': 1,\n",
              "         'and': 61,\n",
              "         'algorithms': 1,\n",
              "         'such': 9,\n",
              "         'as': 25,\n",
              "         'from': 19,\n",
              "         'rnn': 1,\n",
              "         'lstm': 1,\n",
              "         'or': 7,\n",
              "         'gru': 1,\n",
              "         'seq2seq': 1,\n",
              "         'glm': 1,\n",
              "         '20': 1,\n",
              "         'name': 1,\n",
              "         'a': 60,\n",
              "         'few': 1,\n",
              "         'also': 5,\n",
              "         'resulted': 1,\n",
              "         'vastly': 1,\n",
              "         'increasing': 2,\n",
              "         'parameters': 2,\n",
              "         'gpt2': 1,\n",
              "         'with': 14,\n",
              "         '15': 1,\n",
              "         'billion': 2,\n",
              "         'gpt3': 4,\n",
              "         '175': 1,\n",
              "         'google': 1,\n",
              "         'switch': 1,\n",
              "         '16': 1,\n",
              "         'trillion': 1,\n",
              "         'among': 1,\n",
              "         'others': 2,\n",
              "         'however': 6,\n",
              "         'training': 3,\n",
              "         'of': 70,\n",
              "         'scratch': 1,\n",
              "         'requires': 1,\n",
              "         'amounts': 1,\n",
              "         'data': 4,\n",
              "         'computing': 1,\n",
              "         'power': 1,\n",
              "         'far': 1,\n",
              "         'exceeding': 1,\n",
              "         'scope': 1,\n",
              "         'individual': 1,\n",
              "         'application': 1,\n",
              "         'development': 1,\n",
              "         'furthermore': 2,\n",
              "         'while': 6,\n",
              "         'pretrained': 2,\n",
              "         'perform': 3,\n",
              "         'well': 3,\n",
              "         'when': 5,\n",
              "         'applying': 2,\n",
              "         'basic': 2,\n",
              "         'nlp': 7,\n",
              "         'tasks': 6,\n",
              "         'common': 2,\n",
              "         'broadly': 1,\n",
              "         'defined': 3,\n",
              "         'domains': 2,\n",
              "         'they': 5,\n",
              "         'tend': 3,\n",
              "         'not': 7,\n",
              "         'meet': 1,\n",
              "         'requirements': 2,\n",
              "         'more': 14,\n",
              "         'complex': 7,\n",
              "         'applied': 4,\n",
              "         'less': 3,\n",
              "         'narrowly': 1,\n",
              "         'key': 2,\n",
              "         'element': 1,\n",
              "         'supporting': 1,\n",
              "         'wide': 1,\n",
              "         'variety': 1,\n",
              "         'applications': 2,\n",
              "         'is': 35,\n",
              "         'simplicity': 1,\n",
              "         'which': 16,\n",
              "         'may': 4,\n",
              "         'be': 17,\n",
              "         'turned': 1,\n",
              "         'into': 4,\n",
              "         'specialpurpose': 1,\n",
              "         'means': 2,\n",
              "         'finetuning': 11,\n",
              "         'act': 1,\n",
              "         'progressively': 1,\n",
              "         'adapting': 1,\n",
              "         'subset': 4,\n",
              "         'weights': 1,\n",
              "         'based': 2,\n",
              "         'on': 7,\n",
              "         'task': 10,\n",
              "         'domainspecific': 2,\n",
              "         'dataset': 26,\n",
              "         'for': 28,\n",
              "         'example': 2,\n",
              "         'question': 2,\n",
              "         'answering': 2,\n",
              "         'qa': 2,\n",
              "         'obtained': 1,\n",
              "         'using': 9,\n",
              "         'containing': 2,\n",
              "         'pairs': 1,\n",
              "         'questions': 2,\n",
              "         'answers': 2,\n",
              "         'related': 1,\n",
              "         'that': 10,\n",
              "         'domain': 1,\n",
              "         'achieve': 1,\n",
              "         'an': 12,\n",
              "         'acceptable': 1,\n",
              "         'performance': 4,\n",
              "         'it': 6,\n",
              "         'unclear': 1,\n",
              "         'discriminating': 1,\n",
              "         'features': 2,\n",
              "         'able': 4,\n",
              "         'capture': 1,\n",
              "         'what': 2,\n",
              "         'knowledge': 1,\n",
              "         'are': 17,\n",
              "         'generated': 1,\n",
              "         'this': 13,\n",
              "         'makes': 1,\n",
              "         'difficult': 1,\n",
              "         'create': 2,\n",
              "         'particular': 1,\n",
              "         'imposed': 1,\n",
              "         'answer': 2,\n",
              "         'content': 3,\n",
              "         'wording': 3,\n",
              "         'if': 5,\n",
              "         'contexts': 1,\n",
              "         'topics': 1,\n",
              "         'discourse': 1,\n",
              "         'sentiments': 1,\n",
              "         'user': 1,\n",
              "         'education': 1,\n",
              "         'should': 1,\n",
              "         'affect': 1,\n",
              "         'must': 5,\n",
              "         'include': 1,\n",
              "         'all': 4,\n",
              "         'combinations': 2,\n",
              "         'context': 1,\n",
              "         'complexity': 2,\n",
              "         'complicates': 1,\n",
              "         'predictability': 1,\n",
              "         'responses': 3,\n",
              "         'in': 48,\n",
              "         'paper': 6,\n",
              "         'we': 25,\n",
              "         'introduce': 2,\n",
              "         'method': 2,\n",
              "         'building': 1,\n",
              "         'where': 7,\n",
              "         'controllable': 2,\n",
              "         'performed': 4,\n",
              "         'present': 2,\n",
              "         'results': 8,\n",
              "         'initial': 1,\n",
              "         'experiments': 2,\n",
              "         'conducted': 1,\n",
              "         'reduce': 1,\n",
              "         'gender': 34,\n",
              "         'bias': 63,\n",
              "         'english': 1,\n",
              "         'texts': 3,\n",
              "         'suggest': 1,\n",
              "         'further': 2,\n",
              "         'other': 2,\n",
              "         'will': 1,\n",
              "         'reported': 2,\n",
              "         'comprehensive': 1,\n",
              "         'publication': 1,\n",
              "         'rest': 1,\n",
              "         'structured': 1,\n",
              "         'follows': 2,\n",
              "         'section': 4,\n",
              "         '2': 3,\n",
              "         'some': 2,\n",
              "         'notions': 1,\n",
              "         'challenges': 1,\n",
              "         'dealing': 1,\n",
              "         'removal': 7,\n",
              "         'represents': 1,\n",
              "         'us': 1,\n",
              "         'exemplary': 1,\n",
              "         'test': 14,\n",
              "         'our': 12,\n",
              "         'novel': 1,\n",
              "         'approach': 21,\n",
              "         'introduced': 2,\n",
              "         '3': 2,\n",
              "         '4': 1,\n",
              "         'compare': 2,\n",
              "         'two': 6,\n",
              "         'baseline': 1,\n",
              "         'show': 1,\n",
              "         'terms': 11,\n",
              "         'reduction': 1,\n",
              "         '5': 1,\n",
              "         'discuss': 1,\n",
              "         'preliminary': 1,\n",
              "         'envision': 1,\n",
              "         'extension': 1,\n",
              "         'multiple': 4,\n",
              "         'input': 2,\n",
              "         'taken': 1,\n",
              "         'account': 1,\n",
              "         'controlled': 1,\n",
              "         'previous': 1,\n",
              "         'work': 2,\n",
              "         'focused': 1,\n",
              "         'specific': 2,\n",
              "         'aspects': 4,\n",
              "         'word': 9,\n",
              "         'embeddings': 2,\n",
              "         'coreference': 1,\n",
              "         'resolutions': 1,\n",
              "         'partofspeech': 1,\n",
              "         'dependency': 1,\n",
              "         'parsing': 1,\n",
              "         'these': 2,\n",
              "         'approaches': 2,\n",
              "         'share': 1,\n",
              "         'several': 2,\n",
              "         'limitations': 1,\n",
              "         'regarding': 2,\n",
              "         'their': 3,\n",
              "         'effectiveness': 1,\n",
              "         'removing': 1,\n",
              "         'first': 10,\n",
              "         'conflate': 1,\n",
              "         'different': 8,\n",
              "         'conversational': 1,\n",
              "         'dimensions': 3,\n",
              "         'thus': 5,\n",
              "         'unable': 1,\n",
              "         'detect': 1,\n",
              "         'subtle': 2,\n",
              "         'pragmatic': 2,\n",
              "         'differences': 2,\n",
              "         'second': 9,\n",
              "         'often': 1,\n",
              "         'limited': 1,\n",
              "         'explicitly': 2,\n",
              "         'binarly': 1,\n",
              "         'gendered': 4,\n",
              "         'words': 7,\n",
              "         'many': 1,\n",
              "         'third': 3,\n",
              "         'focusing': 1,\n",
              "         'malefemale': 1,\n",
              "         'direction': 3,\n",
              "         'neglect': 1,\n",
              "         'impact': 1,\n",
              "         'orientation': 1,\n",
              "         'but': 3,\n",
              "         'necessarily': 1,\n",
              "         'unfairly': 1,\n",
              "         'biased': 10,\n",
              "         'general': 2,\n",
              "         'framework': 1,\n",
              "         'proposed': 3,\n",
              "         'textual': 1,\n",
              "         'decomposed': 1,\n",
              "         'along': 3,\n",
              "         'three': 5,\n",
              "         'semantic': 1,\n",
              "         '1': 4,\n",
              "         'person': 2,\n",
              "         'being': 4,\n",
              "         'spoken': 2,\n",
              "         'about': 1,\n",
              "         'speaker': 1,\n",
              "         'shown': 2,\n",
              "         'distinction': 1,\n",
              "         'generates': 1,\n",
              "         'better': 1,\n",
              "         'finegrained': 1,\n",
              "         'classifiers': 1,\n",
              "         'consequently': 2,\n",
              "         'define': 2,\n",
              "         'can': 4,\n",
              "         'benefit': 1,\n",
              "         'multistep': 2,\n",
              "         'including': 2,\n",
              "         'detection': 1,\n",
              "         'classification': 6,\n",
              "         'reformulation': 7,\n",
              "         'characterise': 1,\n",
              "         'occurrence': 1,\n",
              "         'one': 7,\n",
              "         'aspect': 2,\n",
              "         'identification': 1,\n",
              "         'type': 17,\n",
              "         'use': 2,\n",
              "         'genderexclusive': 1,\n",
              "         'keywords': 1,\n",
              "         'genderneutral': 2,\n",
              "         'entity': 1,\n",
              "         'explicit': 4,\n",
              "         'reference': 1,\n",
              "         'term': 1,\n",
              "         'through': 2,\n",
              "         'pronoun': 1,\n",
              "         'generalisation': 1,\n",
              "         'expressions': 1,\n",
              "         'meant': 1,\n",
              "         'attitudes': 1,\n",
              "         'towards': 2,\n",
              "         'guided': 1,\n",
              "         'stereotypes': 1,\n",
              "         'benevolent': 4,\n",
              "         'sexism': 4,\n",
              "         'then': 3,\n",
              "         'captures': 1,\n",
              "         'actual': 1,\n",
              "         'having': 2,\n",
              "         'result': 3,\n",
              "         'characterised': 1,\n",
              "         'treatment': 4,\n",
              "         'text': 8,\n",
              "         'combination': 1,\n",
              "         'each': 14,\n",
              "         'pair': 2,\n",
              "         'appropriate': 3,\n",
              "         'break': 1,\n",
              "         'down': 1,\n",
              "         'simpler': 4,\n",
              "         'subtasks': 5,\n",
              "         'subtask': 17,\n",
              "         'created': 3,\n",
              "         'taskspecific': 2,\n",
              "         'finetuned': 5,\n",
              "         'since': 2,\n",
              "         'datasets': 5,\n",
              "         'remain': 1,\n",
              "         'small': 1,\n",
              "         'behaviour': 1,\n",
              "         'once': 2,\n",
              "         'lined': 1,\n",
              "         'up': 1,\n",
              "         'complete': 2,\n",
              "         'overall': 3,\n",
              "         'debiasing': 9,\n",
              "         'extraction': 4,\n",
              "         'dedicated': 2,\n",
              "         'used': 9,\n",
              "         'identify': 1,\n",
              "         'sentence': 8,\n",
              "         'has': 3,\n",
              "         'generalization': 3,\n",
              "         'no': 2,\n",
              "         'detected': 1,\n",
              "         'process': 3,\n",
              "         'halted': 1,\n",
              "         'extract': 1,\n",
              "         'concern': 1,\n",
              "         'there': 1,\n",
              "         'typespecific': 2,\n",
              "         'identified': 3,\n",
              "         'select': 2,\n",
              "         'removed': 3,\n",
              "         'reformulate': 1,\n",
              "         'similar': 1,\n",
              "         'available': 1,\n",
              "         'biascarrying': 1,\n",
              "         'extracted': 2,\n",
              "         'provided': 1,\n",
              "         'addition': 1,\n",
              "         'reformulated': 4,\n",
              "         'contain': 3,\n",
              "         'biases': 3,\n",
              "         'types': 6,\n",
              "         'iterative': 2,\n",
              "         'repeatedly': 1,\n",
              "         'classified': 1,\n",
              "         'treated': 1,\n",
              "         'until': 1,\n",
              "         'biasfree': 1,\n",
              "         'figure': 1,\n",
              "         'figmodel3': 1,\n",
              "         'shows': 1,\n",
              "         'architecture': 3,\n",
              "         'image': 1,\n",
              "         'davinci': 1,\n",
              "         'accessed': 1,\n",
              "         'openai': 1,\n",
              "         'api': 1,\n",
              "         'details': 1,\n",
              "         'refer': 2,\n",
              "         'original': 2,\n",
              "         'temperature': 1,\n",
              "         't': 1,\n",
              "         'set': 4,\n",
              "         '02': 1,\n",
              "         'topp': 1,\n",
              "         'bestof': 1,\n",
              "         'contained': 4,\n",
              "         '10': 2,\n",
              "         'inputoutput': 2,\n",
              "         'examples': 6,\n",
              "         'yielding': 1,\n",
              "         'total': 1,\n",
              "         '40': 2,\n",
              "         'given': 2,\n",
              "         'prompt': 2,\n",
              "         'obtain': 1,\n",
              "         'genderbias': 3,\n",
              "         'wikipedia': 9,\n",
              "         '’': 4,\n",
              "         's': 3,\n",
              "         'neutral': 2,\n",
              "         'point': 1,\n",
              "         'view': 1,\n",
              "         'npov': 12,\n",
              "         'edits': 3,\n",
              "         'edit': 1,\n",
              "         'been': 3,\n",
              "         'therefore': 2,\n",
              "         'changed': 1,\n",
              "         'contributor': 1,\n",
              "         'both': 1,\n",
              "         'versions': 1,\n",
              "         'stored': 1,\n",
              "         'revision': 1,\n",
              "         'history': 1,\n",
              "         'focus': 1,\n",
              "         'sentences': 12,\n",
              "         'did': 2,\n",
              "         'pronouns': 2,\n",
              "         'were': 10,\n",
              "         'excluded': 1,\n",
              "         'determined': 1,\n",
              "         'final': 1,\n",
              "         'size': 2,\n",
              "         '45': 1,\n",
              "         '539': 1,\n",
              "         'sentencepairs': 1,\n",
              "         'comparing': 3,\n",
              "         'unbiased': 8,\n",
              "         'forms': 2,\n",
              "         'biasinducing': 5,\n",
              "         'substitutes': 1,\n",
              "         'finally': 1,\n",
              "         'biaslabeled': 3,\n",
              "         'selected': 1,\n",
              "         'represented': 1,\n",
              "         'equal': 1,\n",
              "         'frequency': 1,\n",
              "         'manually': 1,\n",
              "         'labeled': 2,\n",
              "         'according': 1,\n",
              "         'prompts': 1,\n",
              "         '92': 3,\n",
              "         'composed': 2,\n",
              "         'paired': 3,\n",
              "         'grouped': 2,\n",
              "         'evaluation': 2,\n",
              "         'measurement': 1,\n",
              "         'developed': 1,\n",
              "         'measure': 2,\n",
              "         'before': 1,\n",
              "         'after': 2,\n",
              "         'validates': 1,\n",
              "         'f1': 6,\n",
              "         'score': 3,\n",
              "         'done': 1,\n",
              "         'output': 2,\n",
              "         'expected': 1,\n",
              "         'validated': 1,\n",
              "         'ones': 1,\n",
              "         'because': 1,\n",
              "         'diversity': 1,\n",
              "         'how': 1,\n",
              "         'rephrased': 1,\n",
              "         'classifier': 1,\n",
              "         'again': 1,\n",
              "         'match': 1,\n",
              "         'could': 2,\n",
              "         'still': 1,\n",
              "         'provide': 3,\n",
              "         'indication': 1,\n",
              "         'whether': 1,\n",
              "         'had': 1,\n",
              "         'uses': 1,\n",
              "         'glove': 1,\n",
              "         'quantify': 2,\n",
              "         'following': 2,\n",
              "         'subspace': 2,\n",
              "         'shehe': 1,\n",
              "         'evaluate': 2,\n",
              "         'position': 1,\n",
              "         'typically': 1,\n",
              "         'strong': 1,\n",
              "         'association': 1,\n",
              "         'w': 2,\n",
              "         'computed': 2,\n",
              "         'cosine': 1,\n",
              "         'similarity': 1,\n",
              "         'between': 1,\n",
              "         'its': 2,\n",
              "         'vector': 2,\n",
              "         'representation': 2,\n",
              "         'w⃗': 1,\n",
              "         'vecshe': 1,\n",
              "         'veche': 1,\n",
              "         'degree': 1,\n",
              "         'genderneutrality': 1,\n",
              "         'called': 1,\n",
              "         'neutrality': 4,\n",
              "         'cosvecwvecshe': 1,\n",
              "         'cosvecwveche': 1,\n",
              "         'zero': 3,\n",
              "         'aggregating': 1,\n",
              "         'normalizing': 1,\n",
              "         'whole': 1,\n",
              "         'vocabulary': 3,\n",
              "         'frac1nw': 1,\n",
              "         'sumw': 1,\n",
              "         'wcosvecwvecshe': 1,\n",
              "         'cosvecwveche2': 1,\n",
              "         'nw': 1,\n",
              "         'get': 2,\n",
              "         'namely': 1,\n",
              "         'mean': 3,\n",
              "         'squared': 3,\n",
              "         'mswn': 4,\n",
              "         'designed': 1,\n",
              "         'additional': 2,\n",
              "         'base': 3,\n",
              "         'lines': 2,\n",
              "         'm1': 5,\n",
              "         'consists': 1,\n",
              "         'single': 4,\n",
              "         'm2': 7,\n",
              "         'doubletransformer': 1,\n",
              "         'system': 1,\n",
              "         'identifying': 1,\n",
              "         'selects': 1,\n",
              "         'm3': 8,\n",
              "         'debiased': 6,\n",
              "         'measurements': 1,\n",
              "         'above': 1,\n",
              "         'scores': 3,\n",
              "         'deemed': 1,\n",
              "         'successful': 1,\n",
              "         'replaced': 1,\n",
              "         'alternatives': 1,\n",
              "         'resulting': 1,\n",
              "         'table': 2,\n",
              "         'comparison': 2,\n",
              "         'benefits': 1,\n",
              "         'split': 2,\n",
              "         'emerges': 1,\n",
              "         'clearly': 1,\n",
              "         '100': 1,\n",
              "         'improvement': 2,\n",
              "         'micro': 2,\n",
              "         'averaged': 1,\n",
              "         '50': 1,\n",
              "         '009': 1,\n",
              "         '057': 2,\n",
              "         '087': 2,\n",
              "         '018': 1,\n",
              "         '027': 1,\n",
              "         '095': 1,\n",
              "         '065': 1,\n",
              "         '091': 2,\n",
              "         'average': 1,\n",
              "         '031': 1,\n",
              "         'approch': 1,\n",
              "         'baselines': 1,\n",
              "         'interestingly': 1,\n",
              "         'f1score': 1,\n",
              "         'occur': 1,\n",
              "         'apparently': 1,\n",
              "         'require': 1,\n",
              "         'extensive': 1,\n",
              "         'than': 1,\n",
              "         'case': 2,\n",
              "         'most': 1,\n",
              "         'cases': 1,\n",
              "         'fail': 1,\n",
              "         'corrected': 1,\n",
              "         'even': 1,\n",
              "         'satisfactory': 1,\n",
              "         'improves': 1,\n",
              "         'at': 2,\n",
              "         'stage': 1,\n",
              "         'due': 1,\n",
              "         'fact': 1,\n",
              "         'treating': 1,\n",
              "         'separately': 1,\n",
              "         'allows': 1,\n",
              "         'providing': 1,\n",
              "         'multitransformers': 1,\n",
              "         'presents': 1,\n",
              "         'advantage': 2,\n",
              "         'flexible': 1,\n",
              "         'scalable': 1,\n",
              "         'across': 1,\n",
              "         'level': 1,\n",
              "         'within': 1,\n",
              "         'see': 1,\n",
              "         'values': 1,\n",
              "         'transformerbased': 1,\n",
              "         'contributors': 1,\n",
              "         'comparability': 1,\n",
              "         'considered': 3,\n",
              "         'profession': 1,\n",
              "         'titles': 1,\n",
              "         'descriptive': 1,\n",
              "         'lists': 1,\n",
              "         '49': 1,\n",
              "         'professions': 2,\n",
              "         '67': 1,\n",
              "         'descriptions': 2,\n",
              "         'appear': 1,\n",
              "         '8': 1,\n",
              "         '7': 1,\n",
              "         'resp': 1,\n",
              "         'found': 1,\n",
              "         'closer': 2,\n",
              "         'encountered': 1,\n",
              "         'independent': 1,\n",
              "         'mswns': 1,\n",
              "         'moreover': 2,\n",
              "         'does': 1,\n",
              "         'only': 2,\n",
              "         'apply': 2,\n",
              "         'humans': 2,\n",
              "         '00090': 1,\n",
              "         '00057': 1,\n",
              "         '00049': 1,\n",
              "         '00040': 1,\n",
              "         '00207': 1,\n",
              "         '00085': 1,\n",
              "         '00156': 1,\n",
              "         '00065': 1,\n",
              "         'discussion': 1,\n",
              "         'conclusions': 1,\n",
              "         'presented': 2,\n",
              "         'new': 1,\n",
              "         'address': 1,\n",
              "         'multitude': 1,\n",
              "         'characterize': 1,\n",
              "         'proved': 2,\n",
              "         'favour': 1,\n",
              "         'ultimate': 1,\n",
              "         'specifically': 1,\n",
              "         'effective': 2,\n",
              "         'handful': 1,\n",
              "         'contrast': 1,\n",
              "         'potentially': 1,\n",
              "         'need': 1,\n",
              "         'comparable': 1,\n",
              "         'tasksplitting': 1,\n",
              "         'do': 1,\n",
              "         'possibility': 1,\n",
              "         'generate': 1,\n",
              "         'straightforward': 1,\n",
              "         'extended': 2,\n",
              "         'arbitrary': 1,\n",
              "         'number': 1,\n",
              "         'treatments': 1,\n",
              "         'currently': 1,\n",
              "         'treats': 1,\n",
              "         'iteratively': 1,\n",
              "         'improved': 1,\n",
              "         'would': 1,\n",
              "         'treat': 1,\n",
              "         'natural': 1,\n",
              "         'language': 1,\n",
              "         'indicated': 1,\n",
              "         'order': 1,\n",
              "         'explore': 1,\n",
              "         'applicability': 1,\n",
              "         'settings': 1,\n",
              "         'aim': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xW7U_1_ENx7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}