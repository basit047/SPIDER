{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>Project Module</b></h1></center>\n",
    "<center><h2>**SPIDER: Scientific Publication Indexing and Definition Extraction Resource**</h2></center>\n",
    "\n",
    "\n",
    "<div>\n",
    "\n",
    "<ul style=\"float:left\">\n",
    "<span><b>Group Members</b></span>\n",
    "<li>Pratik Nichite</li>\n",
    "<li>Abdul Basit Raja</li>\n",
    "<li>Pranav Kumar Sah</li>\n",
    "<li>Priyanka Singh</li>\n",
    "</ul>\n",
    "<h4 style=\"float: right\"><b>Project Supervisor</b></br> Prof. Dr. Ivan P. Yamshchikov</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing all the required libraries and modules</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import math # type: ignore\n",
    "import PyPDF2 # type: ignore\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from nltk import pos_tag # type: ignore\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer # type: ignore\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import word_tokenize # type: ignore\n",
    "from pdfminer.high_level import extract_text # type: ignore\n",
    "\n",
    "nltk.data.path.append('/Users/pranav/Project_Module/nltk_data ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_PATH = \"/Users/pranav/Project_Module/Phi-3-mini-128k-instruct\"\n",
    "REMOVAL_PERCENTAGE_VALUE = 0.90 # Percentage Value\n",
    "GLOSSARY_CSV_FILE_PATH = \"/Users/pranav/Project_Module/glossary-statical-learning.csv\"\n",
    "OLLAMA_LOCAL_HOST = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Converting .PDF file to .TXT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path, text_path):\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "\n",
    "            os.makedirs(os.path.dirname(text_path), exist_ok=True)\n",
    "\n",
    "            with open(text_path, 'w', encoding='utf-8') as text_file:\n",
    "                for page_num in range(num_pages):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text_file.write(page.extract_text())\n",
    "\n",
    "        print(f\"Conversion successful. PDF to TXT saved to location: {text_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pdf_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/pranav/Project_Module/An Introduction to Statistical Learning.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Provide your .pdf file location\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/pranav/Project_Module/Converted_Statistical_Learning.txt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Provide your destination path for converted .txt file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pdf_to_text(pdf_file_path,text_file_path)\n",
      "Cell \u001b[0;32mIn[74], line 11\u001b[0m, in \u001b[0;36mpdf_to_text\u001b[0;34m(pdf_path, text_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_pages):\n\u001b[1;32m     10\u001b[0m             page \u001b[38;5;241m=\u001b[39m pdf_reader\u001b[38;5;241m.\u001b[39mpages[page_num]\n\u001b[0;32m---> 11\u001b[0m             text_file\u001b[38;5;241m.\u001b[39mwrite(page\u001b[38;5;241m.\u001b[39mextract_text())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion successful. PDF to TXT saved to location: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PyPDF2/_page.py:1851\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[0;34m(self, Tj_sep, TJ_sep, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, *args)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1849\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[0;32m-> 1851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_text(\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf,\n\u001b[1;32m   1854\u001b[0m     orientations,\n\u001b[1;32m   1855\u001b[0m     space_width,\n\u001b[1;32m   1856\u001b[0m     PG\u001b[38;5;241m.\u001b[39mCONTENTS,\n\u001b[1;32m   1857\u001b[0m     visitor_operand_before,\n\u001b[1;32m   1858\u001b[0m     visitor_operand_after,\n\u001b[1;32m   1859\u001b[0m     visitor_text,\n\u001b[1;32m   1860\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PyPDF2/_page.py:1356\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[0;34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     content \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1353\u001b[0m         obj[content_key]\u001b[38;5;241m.\u001b[39mget_object() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[1;32m   1354\u001b[0m     )\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, ContentStream):\n\u001b[0;32m-> 1356\u001b[0m         content \u001b[38;5;241m=\u001b[39m ContentStream(content, pdf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:  \u001b[38;5;66;03m# it means no content can be extracted(certainly empty page)\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PyPDF2/generic/_data_structures.py:877\u001b[0m, in \u001b[0;36mContentStream.__init__\u001b[0;34m(self, stream, pdf, forced_encoding)\u001b[0m\n\u001b[1;32m    875\u001b[0m     stream_bytes \u001b[38;5;241m=\u001b[39m BytesIO(stream_data_bytes)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforced_encoding \u001b[38;5;241m=\u001b[39m forced_encoding\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__parse_content_stream(stream_bytes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PyPDF2/generic/_data_structures.py:924\u001b[0m, in \u001b[0;36mContentStream.__parse_content_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    922\u001b[0m stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peek\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mor\u001b[39;00m peek \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 924\u001b[0m     operator \u001b[38;5;241m=\u001b[39m read_until_regex(stream, NameObject\u001b[38;5;241m.\u001b[39mdelimiter_pattern, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBI\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# begin inline image - a completely different parsing\u001b[39;00m\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;66;03m# mechanism is required, of course... thanks buddy...\u001b[39;00m\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m operands \u001b[38;5;241m==\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PyPDF2/_utils.py:161\u001b[0m, in \u001b[0;36mread_until_regex\u001b[0;34m(stream, regex, ignore_eof)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m name\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n\u001b[0;32m--> 161\u001b[0m m \u001b[38;5;241m=\u001b[39m regex\u001b[38;5;241m.\u001b[39msearch(tok)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     name \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tok[: m\u001b[38;5;241m.\u001b[39mstart()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pdf_file_path = '/Users/pranav/Project_Module/An Introduction to Statistical Learning.pdf' # Provide your .pdf file location\n",
    "text_file_path = '/Users/pranav/Project_Module/Converted_Statistical_Learning.txt' # Provide your destination path for converted .txt file\n",
    "\n",
    "pdf_to_text(pdf_file_path,text_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the converted .TXT file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(txt_path_location):\n",
    "    with open(txt_path_location, 'r', encoding=\"utf8\") as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining Tags</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has following tags for nouns\n",
    "# NN: Noun, singular or mass\n",
    "# NNS: Noun, plural\n",
    "# NNP: Proper noun, singular\n",
    "# NNPS: Proper noun, plural\n",
    "tag_filter = [\"NN\", \"NNS\"]\n",
    "\n",
    "\n",
    "def filter_tags(input_tag):\n",
    "    token, tag = input_tag\n",
    "    return tag in tag_filter # if the tag is in tags list, it will return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extracting Nouns</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def extract_nouns(input_text):\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', ' ', input_text)\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized_text = word_tokenize(cleaned_text) # nltk tokenizer\n",
    "    \n",
    "    # pos-tagging of stemmed text\n",
    "    pos_tags = pos_tag(tokenized_text)\n",
    "    \n",
    "    # filter out non-Noun tags\n",
    "    filtered_tags = list(filter(filter_tags, pos_tags))\n",
    "    \n",
    "    if len(filtered_tags) <= 0:\n",
    "        return []\n",
    "\n",
    "    tokens, tags = zip(*filtered_tags) #unpacking tokens and tags\n",
    "    \n",
    "    # lemmatization of text\n",
    "    stemmed_tokens = list(map(ps.stem, tokens))\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generating Tokens</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(text):\n",
    "    tokens = []\n",
    "    with open(text_file_path, 'r', encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            temp_tokens = extract_nouns(input_text = line)\n",
    "            tokens.extend(temp_tokens)\n",
    "\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Dictonaries for Frequencies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dictionary(_token):\n",
    "    _frequencies = Counter(_token)\n",
    "    return pd.DataFrame({\"Word\":_frequencies.keys(), \"Frequencies\": _frequencies.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(freq):\n",
    "    df = pd.DataFrame(freq.items(), columns=['Token', 'Frequency'])\n",
    "    df.to_csv(\"Nouns.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sorting Frequencies by value of frequency in Ascending order</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_frequencies_df(_df):\n",
    "    return _df.sort_values(by = [\"Frequency\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Removing most unique data based on percentage value</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_percentage_from_data(_df):\n",
    "    from_index = math.floor(len(_df) * REMOVAL_PERCENTAGE_VALUE)\n",
    "    return _df.iloc[from_index:, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting the Above Generated Frequencies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_frequencies(_frequencies):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(_frequencies.keys(), _frequencies.values())\n",
    "    plt.ylabel('token frequencies')\n",
    "    plt.xlabel('tokens')\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Word Frequencies (Book)\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using Phi3 Model for Tokenizing the text from Hugging Face</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenizing the data by the Phi3</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df):\n",
    "    \n",
    "    data_tokenized = []\n",
    "    nouns = df[\"Token\"].values\n",
    "\n",
    "    for noun in nouns:  \n",
    "        tokens = tokenizer.tokenize(noun)\n",
    "\n",
    "        num_tokens = len(tokens)\n",
    "        word_length = len(noun)\n",
    "\n",
    "        data_tokenized.append({\"tokens\": tokens, \"num_tokens\": num_tokens, \"word_length\": word_length})\n",
    "\n",
    "    return data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratio_num_token_for_word_length(_df):\n",
    "    _df[\"ratio\"] = _df[\"num_tokens\"] / _df[\"word_length\"]\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating Normal Distribution\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-(x - mu)**2 / (2 * sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_sd_of_tokenized_data(_df):\n",
    "    mean = _df['ratio'].mean()\n",
    "    std = _df['ratio'].std()\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting Graph to show Distribution</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(_df, mean, std):\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "\n",
    "    x = np.linspace(_df['ratio'].min(), _df['ratio'].max(), 1000)\n",
    "    y = normal_pdf(x, mean, std)\n",
    "\n",
    "    # normal distribution\n",
    "    plt.plot(x, y, 'blue', lw=2) \n",
    "\n",
    "    #histogram\n",
    "    plt.hist(_df[\"ratio\"], bins=13, density=True, color=\"green\", alpha=0.6, edgecolor='black')\n",
    "\n",
    " \n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.title(\"Distribution of Tokens\")\n",
    "    plt.xlabel(\"Ratio (#Tokens / #Letters)\")\n",
    "    plt.ylabel(\"Population\")\n",
    "\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_terms_by_threshold_min_max_value(df, threshold_min, threshold_max):\n",
    "    unique_terms = df.loc[df['ratio'] <= threshold_max].copy()\n",
    "    unique_terms = unique_terms.loc[unique_terms['ratio'] > threshold_min]\n",
    "    unique_terms.to_csv(f\"{threshold_min-threshold_max}.csv\")\n",
    "    return unique_terms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>additive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>additivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alternative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hypothesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>weights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>within</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>covariance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>wrapper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>758 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           words\n",
       "0     activation\n",
       "1       additive\n",
       "2     additivity\n",
       "3    alternative\n",
       "4     hypothesis\n",
       "..           ...\n",
       "753      weights\n",
       "754       within\n",
       "755        class\n",
       "756   covariance\n",
       "757      wrapper\n",
       "\n",
       "[758 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glossary = pd.read_csv(GLOSSARY_CSV_FILE_PATH)\n",
    "df_glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns_from_glossary(_df):\n",
    "    glossary = _df[\"words\"].dropna().values\n",
    "    glossary_tk_spider = []\n",
    "\n",
    "# tokenize with our process first\n",
    "    for word in glossary:\n",
    "        tokens = extract_nouns(word)\n",
    "        if len(tokens) <= 0:\n",
    "            continue\n",
    "        \n",
    "        glossary_tk_spider.extend(tokens)\n",
    "    return glossary_tk_spider\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activ', 'addit', 'altern', 'hypothesi', 'area', 'curv', 'array', 'auc', 'auto', 'correl', 'backfit', 'backward', 'stepwis', 'select', 'bag', 'word', 'baselin', 'classifi', 'decis', 'boundari', 'error', 'regress', 'tree', 'procedur', 'subset', 'select', 'bia', 'bia', 'varianc', 'trade', 'bikeshar', 'data', 'set', 'binari', 'method', 'boolean', 'bootstrap', 'bottom', 'braincanc', 'data', 'set', 'burn', 'caravan', 'data', 'set', 'carseat', 'data', 'set', 'data', 'left', 'time', 'chain', 'rule', 'cifar', 'data', 'set', 'classif', 'tree', 'classifi', 'bottom', 'mean', 'colleg', 'data', 'set', 'collinear', 'concaten', 'contour', 'contrast', 'convolut', 'layer', 'convolut', 'network', 'correl', 'cox', 'proport', 'hazard', 'model', 'cp', 'cross', 'entropi', 'cross', 'valid', 'data', 'set', 'auto', 'caravan', 'carseat', 'cifar', 'colleg', 'credit', 'default', 'heart', 'hitter', 'mnist', 'portfolio', 'public', 'smarket', 'usarrest', 'data', 'type', 'index', 'dendrogram', 'depend', 'detector', 'layer', 'distanc', 'distribut', 'dropout', 'degre', 'freedom', 'eigen', 'decomposit', 'elbow', 'layer', 'entropi', 'rate', 'term', 'event', 'time', 'valu', 'exploratori', 'data', 'analysi', 'f', 'factor', 'failur', 'time', 'rate', 'famili', 'wise', 'error', 'rate', 'featur', 'featur', 'select', 'feed', 'network', 'figur', 'valu', 'stepwis', 'select', 'fund', 'data', 'set', 'distribut', 'gener', 'model', 'gradient', 'concord', 'index', 'hazard', 'function', 'heart', 'data', 'set', 'heteroscedast', 'hidden', 'layer', 'principl', 'dimension', 'hing', 'loss', 'holm', 'method', 'hypergeometr', 'distribut', 'hyperparamet', 'hyperplan', 'imdb', 'data', 'set', 'incom', 'data', 'set', 'increment', 'indic', 'function', 'inner', 'product', 'input', 'interact', 'interpol', 'invers', 'joint', 'distribut', 'classifi', 'regress', 'kaplan', 'surviv', 'curv', 'non', 'linear', 'polynomi', 'radial', 'data', 'set', 'knot', 'l', 'norm', 'laplac', 'distribut', 'leaf', 'squar', 'level', 'likelihood', 'function', 'linear', 'combin', 'linear', 'discrimin', 'analysi', 'linear', 'kernel', 'linear', 'regress', 'simpl', 'link', 'function', 'linkag', 'list', 'index', 'log', 'odd', 'regress', 'multinomi', 'loss', 'function', 'dimension', 'lstm', 'rnn', 'major', 'vote', 'mantel', 'haenszel', 'test', 'distribut', 'markov', 'chain', 'carlo', 'matrix', 'complet', 'classifi', 'maximum', 'likelihood', 'mean', 'error', 'method', 'mnist', 'data', 'set', 'model', 'select', 'multicollinear', 'multinomi', 'regress', 'multipl', 'multivari', 'namespac', 'predict', 'valu', 'network', 'termin', 'nois', 'non', 'parametr', 'notebook', 'rate', 'odd', 'standard', 'error', 'rule', 'versu', 'hyperplan', 'optim', 'train', 'error', 'outlier', 'output', 'overdispers', 'p', 'valu', 'paramet', 'path', 'algorithm', 'permut', 'poisson', 'distribut', 'poisson', 'regress', 'polynomi', 'regress', 'popul', 'regress', 'line', 'portfolio', 'data', 'set', 'distribut', 'mode', 'probabl', 'interv', 'predictor', 'analysi', 'vector', 'regress', 'scree', 'plot', 'probabl', 'project', 'link', 'public', 'data', 'set', 'python', 'object', 'function', 'captur', 'loc', 'anova', 'lm', 'biplot', 'bs', 'bspline', 'clone', 'column', 'drop', 'comput', 'linkag', 'confus', 'tabl', 'contour', 'cumsum', 'dataset', 'decis', 'function', 'decisiontreeclassifi', 'dendrogram', 'describ', 'index', 'dropna', 'elasticnetcv', 'export', 'text', 'fit', 'fit', 'transform', 'gaussiannb', 'regressor', 'dummi', 'influenc', 'predict', 'rdataset', 'glm', 'gridsearchcv', 'groupbi', 'iloc', 'imshow', 'islp', 'bart', 'islp', 'cluster', 'kmean', 'lambda', 'analysi', 'legend', 'lineargam', 'index', 'mean', 'min', 'modelspec', 'ms', 'modelspec', 'mult', 'test', 'multipletest', 'multipletest', 'multipletest', 'multivari', 'logrank', 'test', 'naturalsplin', 'ndim', 'np', 'np', 'allclos', 'np', 'arang', 'np', 'array', 'np', 'concaten', 'np', 'corrcoef', 'np', 'np', 'ix', 'np', 'linalg', 'svd', 'np', 'linspac', 'np', 'logspac', 'np', 'mean', 'np', 'np', 'percentil', 'np', 'power', 'np', 'random', 'choic', 'np', 'random', 'default', 'rng', 'np', 'squeez', 'np', 'std', 'np', 'sum', 'np', 'var', 'np', 'ns', 'os', 'chdir', 'param', 'pca', 'pd', 'crosstab', 'plot', 'plot', 'scatter', 'plot', 'gam', 'plot', 'svm', 'plsregress', 'poli', 'predict', 'print', 'pygam', 'pytorch', 'analysi', 'quadraticdiscriminantanalysi', 'randomforestregressor', 'rf', 'randomforestregressor', 'rng', 'np', 'random', 'default', 'rng', 'rng', 'choic', 'rng', 'standard', 'roc', 'curv', 'roccurvedisplay', 'estim', 'scatter', 'scipi', 'interpol', 'set', 'titl', 'set', 'xscale', 'set', 'yscale', 'shufflesplit', 'skl', 'sklearn', 'linear', 'model', 'skl', 'elasticnet', 'skl', 'elasticnet', 'path', 'skl', 'elasticnet', 'path', 'sklearn', 'sklearn', 'sklearn', 'linear', 'model', 'sklearn', 'model', 'select', 'sklearn', 'sklearn', 'select', 'path', 'sklearn', 'sm', 'sm', 'statsmodel', 'sm', 'ol', 'standardscal', 'stepwis', 'str', 'contain', 'subplot', 'supportvectorregress', 'svc', 'supportvector', 'classifi', 'svr', 'supportvector', 'torch', 'tupl', 'var', 'varianc', 'inflat', 'factor', 'vif', 'varianc', 'inflat', 'zip', 'index', 'random', 'seed', 'recal', 'recommend', 'system', 'recurr', 'network', 'recurs', 'binari', 'split', 'regress', 'tree', 'relu', 'standard', 'error', 'sum', 'squar', 'residu', 'ridg', 'regress', 'robust', 'rug', 'plot', 'elbow', 'sequenc', 'penalti', 'sigmoid', 'signatur', 'slice', 'smarket', 'data', 'set', 'spline', 'margin', 'classifi', 'spars', 'specif', 'spline', 'linear', 'regress', 'thin', 'plate', 'model', 'string', 'subset', 'select', 'subtre', 'classifi', 'machin', 'regress', 'surviv', 'synergi', 'test', 'sampl', 'test', 'mse', 'observ', 'set', 'distribut', 'time', 'seri', 'train', 'train', 'error', 'mse', 'tree', 'tree', 'method', 'rate', 'power', 'basi', 'tukey', 'method', 'paramet', 'sampl', 'test', 'type', 'error', 'rate', 'index', 'usarrest', 'data', 'set', 'valid', 'set', 'approach', 'dummi', 'import', 'indic', 'output', 'varianc', 'coeffici', 'model', 'learner', 'squar', 'weight', 'class', 'covari', 'wrapper']\n"
     ]
    }
   ],
   "source": [
    "glossary_tk_spider = extract_nouns_from_glossary(df_glossary)\n",
    "print(glossary_tk_spider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_glossary(_df):\n",
    "    glossary_tokenized = []\n",
    "\n",
    "# apply phi3 tokenizer\n",
    "    for word in _df:  \n",
    "        tokens = tokenizer.tokenize(word)\n",
    "        glossary_tokenized.append(tokens)\n",
    "    return glossary_tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Common Data Between Glossary and SPIDER's Output\n",
    "def get_intersection(list_1, list_2):\n",
    "    matching_words = []\n",
    "    for word in list_2:\n",
    "        if word in list_1:\n",
    "            matching_words.append(word)\n",
    "\n",
    "    print(f\"total words in SPIDER output: {len(list_1)}\")\n",
    "    print(f\"total words in glossary: {len(list_2)}\")\n",
    "    print(f\"total words in intersection: {len(matching_words)}\")\n",
    "    print(f\"intersection of SPIDER output & glossary: {matching_words}\")\n",
    "    print(\"=\" * 80)\n",
    "    return len(matching_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.1),\n",
       " (0.0, 0.2),\n",
       " (0.0, 0.3),\n",
       " (0.0, 0.4),\n",
       " (0.0, 0.5),\n",
       " (0.0, 0.6),\n",
       " (0.0, 0.7),\n",
       " (0.0, 0.8),\n",
       " (0.0, 0.9),\n",
       " (0.1, 0.2),\n",
       " (0.1, 0.3),\n",
       " (0.1, 0.4),\n",
       " (0.1, 0.5),\n",
       " (0.1, 0.6),\n",
       " (0.1, 0.7),\n",
       " (0.1, 0.8),\n",
       " (0.1, 0.9),\n",
       " (0.2, 0.3),\n",
       " (0.2, 0.4),\n",
       " (0.2, 0.5),\n",
       " (0.2, 0.6),\n",
       " (0.2, 0.7),\n",
       " (0.2, 0.8),\n",
       " (0.2, 0.9),\n",
       " (0.3, 0.4),\n",
       " (0.3, 0.5),\n",
       " (0.3, 0.6),\n",
       " (0.3, 0.7),\n",
       " (0.3, 0.8),\n",
       " (0.3, 0.9),\n",
       " (0.4, 0.5),\n",
       " (0.4, 0.6),\n",
       " (0.4, 0.7),\n",
       " (0.4, 0.8),\n",
       " (0.4, 0.9),\n",
       " (0.5, 0.6),\n",
       " (0.5, 0.7),\n",
       " (0.5, 0.8),\n",
       " (0.5, 0.9),\n",
       " (0.6, 0.7),\n",
       " (0.6, 0.8),\n",
       " (0.6, 0.9),\n",
       " (0.7, 0.8),\n",
       " (0.7, 0.9),\n",
       " (0.8, 0.9)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Threshold combination for recall\n",
    "def generate_threshold_combinations():\n",
    "    values_0_to_1 = np.arange(0, 1, 0.1)\n",
    "\n",
    "  # Convert the arrays to lists for easier manipulation\n",
    "    values_0_to_1_list = values_0_to_1.tolist()\n",
    "\n",
    "  # Generate all combinations of tuples with two values\n",
    "    combinations = [(np.round(x, decimals=1), np.round(y, decimals=1)) for x in values_0_to_1_list for y in values_0_to_1_list if x < y]\n",
    "    return combinations\n",
    "\n",
    "generate_threshold_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping Through each combination to find best range for recall\n",
    "\n",
    "def getString(tokens):\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "def plot_intersection_union_data(_dt_tokenized, _dt_glossary_tokenized):\n",
    "    glossary_terms_list = list(map(getString, _dt_glossary_tokenized))\n",
    "\n",
    "    threshold = generate_threshold_combinations()\n",
    "    intersection_values = []\n",
    "\n",
    "    for i,j in threshold:\n",
    "        unique_terms_df = get_unique_terms_by_threshold_min_max_value(_dt_tokenized, threshold_min=i, threshold_max=j)\n",
    "        unique_terms = unique_terms_df[\"tokens\"].values\n",
    "        unique_terms_list = list(map(getString, unique_terms))\n",
    "  \n",
    "        intersection_value = get_intersection(unique_terms_list, glossary_terms_list)\n",
    "        intersection_values.append({\"intersection-value\": intersection_value, \"min-threshold\": i, \"max-threshold\": j})\n",
    "        \n",
    "    iv = [d[\"intersection-value\"] for d in intersection_values]\n",
    "    min_thresholds = [d[\"min-threshold\"] for d in intersection_values]\n",
    "    max_thresholds = [d[\"max-threshold\"] for d in intersection_values]\n",
    "\n",
    "    # Combine min and max thresholds into a single label\n",
    "    labels = [f'Min: {min}, Max: {max}' for min, max in zip(min_thresholds, max_thresholds)]\n",
    "\n",
    "    # Plot a horizontal bar chart for each intersection value\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, value in enumerate(iv):\n",
    "        plt.barh(labels[i], value, align='center', alpha=0.7, color='blue')\n",
    " \n",
    "    plt.title('Intersection Values vs. Thresholds')\n",
    "    plt.xlabel('Intersection Value')\n",
    "    plt.ylabel('Thresholds')\n",
    "    plt.xticks([x for x in range(0, 500, 50)])\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match the orientation of the labels\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plotting Recall values for each combination</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_string(df):\n",
    "    if len(df) > 1:\n",
    "        csv_string = ', '.join(df)\n",
    "        \n",
    "    return csv_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using Phi3 on Ollama locally to extract defintion for the most unique words</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definition(prompt, is_empty_request = False, model_name = \"phi3\"):\n",
    "    # data\n",
    "    url = OLLAMA_LOCAL_HOST\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    query = {\n",
    "    \"model\": model_name,\n",
    "    **({\"prompt\": prompt, \"keep_alive\": \"10m\", \"stream\": False} if not is_empty_request else {})\n",
    "}\n",
    "    \n",
    "    # http request\n",
    "    response = requests.post(url, headers=headers, json=query)\n",
    "\n",
    "    # response\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result[\"response\"]\n",
    "    else:\n",
    "        return f\"Error {response.status_code}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Optimizing Ollama by empty request https://github.com/ollama/ollama/blob/main/docs/faq.md</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_pdf_to_txt(path):\n",
    "    return path.replace(\"pdf\", \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SPIDER(_file_path, model_name, is_file_pdf = False):\n",
    "    \n",
    "    _txt_file_path = \"\"\n",
    "    \n",
    "    # Convert file to .txt\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if is_file_pdf:\n",
    "      print(\"Converting .pdf to .txt\")\n",
    "      _txt_file_path = set_pdf_to_txt(_file_path)\n",
    "      pdf_to_text(pdf_file_path, _txt_file_path)\n",
    "    else:\n",
    "      _txt_file_path = _file_path\n",
    "      \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Reading .txt\")\n",
    "    # Read the converted .txt\n",
    "    _text = read_txt_file(_txt_file_path)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Generating Tokens\")\n",
    "    # Generate token     \n",
    "    _tokens = generate_token(_text)\n",
    "    print(_tokens)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Converting Tokens to Dictionary\")\n",
    "    # Convert to Dictionary\n",
    "    _frequencies = Counter(_tokens)\n",
    "    pd.DataFrame({\"Word\":_frequencies.keys(), \"Frequencies\": _frequencies.values()})\n",
    "    print(pd.DataFrame({\"Word\":_frequencies.keys(), \"Frequencies\": _frequencies.values()}))\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Generating Frequencies\")\n",
    "    # Generate Frequencies\n",
    "    _frequencies_df = generate_frequencies(_frequencies)\n",
    "    print(_frequencies_df)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Sorting Frequencies\")\n",
    "    # Sort Frequencies\n",
    "    _df_test_frequencies = _frequencies_df\n",
    "    _df_test_frequencies = sort_frequencies_df(_df_test_frequencies)\n",
    "    print(_df_test_frequencies)\n",
    "    \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Remove some data\")\n",
    "    # Removing most unique data based on percentage value\n",
    "    _df_test_frequencies = remove_percentage_from_data(_df_test_frequencies) \n",
    "    \n",
    "    print(_df_test_frequencies)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Generating Plot\")\n",
    "    \n",
    "    # Plot the Frequencies\n",
    "    plot_generated_frequencies(_frequencies)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Tokeinzing Data\") \n",
    "    \n",
    "    # Tokenize Data\n",
    "    _data_tokenized = tokenize_data(_df_test_frequencies)\n",
    "    _df_data_tokenized = pd.DataFrame(_data_tokenized)\n",
    "    print(_df_data_tokenized)\n",
    "    \n",
    "    # Calculate Ration of Num_token\n",
    "    _df_data_tokenized = calculate_ratio_num_token_for_word_length(_df_data_tokenized)\n",
    "\n",
    "    # Generating Min and Max Ratio\n",
    "    _df_data_tokenized[\"ratio\"].min(), _df_data_tokenized[\"ratio\"].max()\n",
    "    \n",
    "    \n",
    "    # Calculating Mean and Standard Deviation of Tokenized Data\n",
    "    _mean, _std = calculate_mean_sd_of_tokenized_data(_df_data_tokenized)\n",
    "\n",
    "    print(f\"mean:{_mean:.2f}, std:{_std:.2f}\")\n",
    "    \n",
    "    # Plot Distibution\n",
    "    plot_distribution(_df_data_tokenized, _mean, _std)\n",
    "\n",
    "    \n",
    "   \n",
    "    # Read Glossary\n",
    "    _df_glossary = pd.read_csv(GLOSSARY_CSV_FILE_PATH)\n",
    "    print(_df_glossary)\n",
    "    \n",
    "   \n",
    "    # Extract Noun from Glossary \n",
    "    _glossary_tk_spider = extract_nouns_from_glossary(_df_glossary)\n",
    "    \n",
    "    # Tokenize Glossary\n",
    "    _glossary_tokenized = tokenize_glossary(_glossary_tk_spider)\n",
    "    print(_glossary_tokenized)\n",
    "\n",
    "    # Plot Intersection/Common Data\n",
    "    plot_intersection_union_data(_df_data_tokenized, _glossary_tokenized)\n",
    "   \n",
    "    \n",
    "    _df_txt_to_pass_llm = _df_test_frequencies.iloc[:,0]\n",
    "    \n",
    "    \n",
    "    _csv_string = convert_df_to_string(_df_txt_to_pass_llm)\n",
    "    return _csv_string, model_name\n",
    "    #print(_csv_string)\n",
    "\n",
    "    # df = pd.DataFrame({\"Word\": _csv_string.split(\",\")})\n",
    "    # df\n",
    "    \n",
    "    # prompt = f\"I have a book that is related to Statistics, can you extract the unique words, give one line definition of the unique words and please remove the words which are not in context of Statistics from this list:  {_csv_string}\"\n",
    "\n",
    "    # response = get_definition(prompt, model_name)\n",
    "    # print(\"test\")\n",
    "    # print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_type(file_extension):\n",
    "    if file_extension == 1:\n",
    "        return 'PDF'\n",
    "    elif file_extension == 2:\n",
    "        return 'TXT'\n",
    "    else:\n",
    "        return 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(type):\n",
    "    if type == 1:\n",
    "        return 'Phi3'\n",
    "    elif type == 2:\n",
    "        return 'Mistral'\n",
    "    else:\n",
    "        return 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_console_layer():\n",
    "    print(\"Welcome to SPIDER Application\")\n",
    "\n",
    "    try:\n",
    "        input_file_extension = int(input(\"Enter the file type of your file, if .pdf enter 1, if .txt enter 2: \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter 1 for .pdf or 2 for .txt\")\n",
    "        return\n",
    "\n",
    "    file_type = get_file_type(input_file_extension)\n",
    "    \n",
    "    if file_type == 'UNKNOWN':\n",
    "        print(\"Unsupported file type. Please enter 1 for .pdf or 2 for .txt\")\n",
    "        return\n",
    "\n",
    "    input_file_path = input(f\"Enter the {file_type} file path/location: \")\n",
    "    \n",
    "    if not os.path.isfile(input_file_path):\n",
    "        print(\"File does not exist or invalid path\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_model_type = int(input(\"Which model you want to use? Press 1 for Phi3, Press 2 for Mistral: \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter 1 for Phi3 or 2 for Mistral\")\n",
    "        return\n",
    "\n",
    "    model_type = get_model_type(input_model_type)\n",
    "    \n",
    "    if model_type == 'UNKNOWN':\n",
    "        print(\"Unsupported model type. Please enter 1 for Phi3 or 2 for Mistral\")\n",
    "        return\n",
    "\n",
    "    str1, model_name = SPIDER('/Users/pranav/Project_Module/An Introduction to Statistical Learning.pdf', model_type, True)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Generating Definitions\")\n",
    "\n",
    "    prompt = f\"I have a book that is related to Statistics, can you extract the unique words, give one line definition of the unique words and please remove the words which are not in context of Statistics from this list:  {str1}\"\n",
    "\n",
    "    response = get_definition(prompt)\n",
    "    print(response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to SPIDER Application\n"
     ]
    }
   ],
   "source": [
    "# '/Users/pranav/Project_Module/An Introduction to Statistical Learning.pdf'\n",
    "model_output = run_console_layer()\n",
    "with open('Generated_Defintion.txt','w') as f:\n",
    "        f.write(model_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
